{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BCyhjd33Eto7"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76WV8FiREto-"
      },
      "source": [
        "**Introduction** \\|\\| [Tensors](tensors_deeper_tutorial.html) \\|\\|\n",
        "[Autograd](autogradyt_tutorial.html) \\|\\| [Building\n",
        "Models](modelsyt_tutorial.html) \\|\\| [TensorBoard\n",
        "Support](tensorboardyt_tutorial.html) \\|\\| [Training\n",
        "Models](trainingyt.html) \\|\\| [Model Understanding](captumyt.html)\n",
        "\n",
        "Introduction to PyTorch\n",
        "=======================\n",
        "\n",
        "Follow along with the video below or on\n",
        "[youtube](https://www.youtube.com/watch?v=IC0_FRiX-sw).\n",
        "\n",
        "``` {.python .jupyter-code-cell}\n",
        "from IPython.display import display, HTML\n",
        "html_code = \"\"\"\n",
        "<div style=\"margin-top:10px; margin-bottom:10px;\">\n",
        "  <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IC0_FRiX-sw\" frameborder=\"0\" allow=\"accelerometer; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
        "</div>\n",
        "\"\"\"\n",
        "display(HTML(html_code))\n",
        "```\n",
        "\n",
        "PyTorch Tensors\n",
        "---------------\n",
        "\n",
        "Follow along with the video beginning at\n",
        "[03:50](https://www.youtube.com/watch?v=IC0_FRiX-sw&t=230s).\n",
        "\n",
        "First, we'll import pytorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1R8hE3GYEto_"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKZs5GSCEtpA"
      },
      "source": [
        "Let's see a few basic tensor manipulations. First, just a few of the\n",
        "ways to create tensors:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rC_BxbIAEtpA",
        "outputId": "b365f7fe-b500-4d65-f95b-acf557b0ebee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "torch.float32\n"
          ]
        }
      ],
      "source": [
        "z = torch.zeros(4, 4)\n",
        "print(z)\n",
        "print(z.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVOvYtLfEtpA"
      },
      "source": [
        "Above, we create a 5x3 matrix filled with zeros, and query its datatype\n",
        "to find out that the zeros are 32-bit floating point numbers, which is\n",
        "the default PyTorch.\n",
        "\n",
        "What if you wanted integers instead? You can always override the\n",
        "default:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGEtNl1iEtpA",
        "outputId": "b0dca4b1-a41a-4153-832a-10b5ca889438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 1, 1],\n",
            "        [1, 1, 1],\n",
            "        [1, 1, 1],\n",
            "        [1, 1, 1],\n",
            "        [1, 1, 1]], dtype=torch.int16)\n"
          ]
        }
      ],
      "source": [
        "i = torch.ones((5, 3), dtype=torch.int16)\n",
        "print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1B3hKM8EtpB"
      },
      "source": [
        "You can see that when we do change the default, the tensor helpfully\n",
        "reports this when printed.\n",
        "\n",
        "It's common to initialize learning weights randomly, often with a\n",
        "specific seed for the PRNG for reproducibility of results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zsdqpILEtpB",
        "outputId": "62a4e643-2953-43d1-e8d2-0822118e7524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A random tensor:\n",
            "tensor([[0.3126, 0.3791],\n",
            "        [0.3087, 0.0736]])\n",
            "\n",
            "A different random tensor:\n",
            "tensor([[0.4216, 0.0691],\n",
            "        [0.2332, 0.4047]])\n",
            "\n",
            "Should match r1:\n",
            "tensor([[0.3126, 0.3791],\n",
            "        [0.3087, 0.0736]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1729)\n",
        "r1 = torch.rand(2, 2)\n",
        "print('A random tensor:')\n",
        "print(r1)\n",
        "\n",
        "r2 = torch.rand(2, 2)\n",
        "print('\\nA different random tensor:')\n",
        "print(r2) # new values\n",
        "\n",
        "torch.manual_seed(1729)\n",
        "r3 = torch.rand(2, 2)\n",
        "print('\\nShould match r1:')\n",
        "print(r3) # repeats values of r1 because of re-seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G98TD99EtpB"
      },
      "source": [
        "PyTorch tensors perform arithmetic operations intuitively. Tensors of\n",
        "similar shapes may be added, multiplied, etc. Operations with scalars\n",
        "are distributed over the tensor:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0CaiuTGEtpC",
        "outputId": "be0d5274-3a21-49d4-b04b-720d7047d48e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.]])\n",
            "tensor([[3., 3., 3.],\n",
            "        [3., 3., 3.]])\n",
            "torch.Size([2, 3])\n"
          ]
        }
      ],
      "source": [
        "ones = torch.ones(2, 3)\n",
        "print(ones)\n",
        "\n",
        "twos = torch.ones(2, 3) * 2 # every element is multiplied by 2\n",
        "print(twos)\n",
        "\n",
        "threes = ones + twos       # addition allowed because shapes are similar\n",
        "print(threes)              # tensors are added element-wise\n",
        "print(threes.shape)        # this has the same dimensions as input tensors\n",
        "\n",
        "r1 = torch.rand(2, 3)\n",
        "r2 = torch.rand(3, 2)\n",
        "# uncomment this line to get a runtime error\n",
        "# r3 = r1 + r2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23XRx00SEtpC"
      },
      "source": [
        "Here's a small sample of the mathematical operations available:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t27IuUeAEtpC",
        "outputId": "68274ad7-a798-46ab-bec9-dc5f7ef184ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A random matrix, r:\n",
            "tensor([[ 0.9956, -0.2232],\n",
            "        [ 0.3858, -0.6593]])\n",
            "\n",
            "Absolute value of r:\n",
            "tensor([[0.9956, 0.2232],\n",
            "        [0.3858, 0.6593]])\n",
            "\n",
            "Inverse sine of r:\n",
            "tensor([[ 1.4775, -0.2251],\n",
            "        [ 0.3961, -0.7199]])\n",
            "\n",
            "Determinant of r:\n",
            "tensor(-0.5703)\n",
            "\n",
            "Singular value decomposition of r:\n",
            "torch.return_types.svd(\n",
            "U=tensor([[-0.8353, -0.5497],\n",
            "        [-0.5497,  0.8353]]),\n",
            "S=tensor([1.1793, 0.4836]),\n",
            "V=tensor([[-0.8851, -0.4654],\n",
            "        [ 0.4654, -0.8851]]))\n",
            "\n",
            "Average and standard deviation of r:\n",
            "(tensor(0.7217), tensor(0.1247))\n",
            "\n",
            "Maximum value of r:\n",
            "tensor(0.9956)\n"
          ]
        }
      ],
      "source": [
        "r = (torch.rand(2, 2) - 0.5) * 2 # values between -1 and 1\n",
        "print('A random matrix, r:')\n",
        "print(r)\n",
        "\n",
        "# Common mathematical operations are supported:\n",
        "print('\\nAbsolute value of r:')\n",
        "print(torch.abs(r))\n",
        "\n",
        "# ...as are trigonometric functions:\n",
        "print('\\nInverse sine of r:')\n",
        "print(torch.asin(r))\n",
        "\n",
        "# ...and linear algebra operations like determinant and singular value decomposition\n",
        "print('\\nDeterminant of r:')\n",
        "print(torch.det(r))\n",
        "print('\\nSingular value decomposition of r:')\n",
        "print(torch.svd(r))\n",
        "\n",
        "# ...and statistical and aggregate operations:\n",
        "print('\\nAverage and standard deviation of r:')\n",
        "print(torch.std_mean(r))\n",
        "print('\\nMaximum value of r:')\n",
        "print(torch.max(r))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-VGuMoVEtpC"
      },
      "source": [
        "There's a good deal more to know about the power of PyTorch tensors,\n",
        "including how to set them up for parallel computations on GPU - we'll be\n",
        "going into more depth in another video.\n",
        "\n",
        "PyTorch Models\n",
        "==============\n",
        "\n",
        "Follow along with the video beginning at\n",
        "[10:00](https://www.youtube.com/watch?v=IC0_FRiX-sw&t=600s).\n",
        "\n",
        "Let's talk about how we can express models in PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dqyQmO0YEtpD"
      },
      "outputs": [],
      "source": [
        "import torch                     # for all things PyTorch\n",
        "import torch.nn as nn            # for torch.nn.Module, the parent object for PyTorch models\n",
        "import torch.nn.functional as F  # for the activation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8awrMMMsEtpD"
      },
      "source": [
        "![](https://pytorch.org/tutorials/_static/img/mnist.png)\n",
        "\n",
        "*Figure: LeNet-5*\n",
        "\n",
        "Above is a diagram of LeNet-5, one of the earliest convolutional neural\n",
        "nets, and one of the drivers of the explosion in Deep Learning. It was\n",
        "built to read small images of handwritten numbers (the MNIST dataset),\n",
        "and correctly classify which digit was represented in the image.\n",
        "\n",
        "Here's the abridged version of how it works:\n",
        "\n",
        "-   Layer C1 is a convolutional layer, meaning that it scans the input\n",
        "    image for features it learned during training. It outputs a map of\n",
        "    where it saw each of its learned features in the image. This\n",
        "    \"activation map\" is downsampled in layer S2.\n",
        "-   Layer C3 is another convolutional layer, this time scanning C1's\n",
        "    activation map for *combinations* of features. It also puts out an\n",
        "    activation map describing the spatial locations of these feature\n",
        "    combinations, which is downsampled in layer S4.\n",
        "-   Finally, the fully-connected layers at the end, F5, F6, and OUTPUT,\n",
        "    are a *classifier* that takes the final activation map, and\n",
        "    classifies it into one of ten bins representing the 10 digits.\n",
        "\n",
        "How do we express this simple neural network in code?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "y1Muct1qEtpD"
      },
      "outputs": [],
      "source": [
        "class LeNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqa83MkVEtpD"
      },
      "source": [
        "Looking over this code, you should be able to spot some structural\n",
        "similarities with the diagram above.\n",
        "\n",
        "This demonstrates the structure of a typical PyTorch model:\n",
        "\n",
        "-   It inherits from `torch.nn.Module` - modules may be nested - in\n",
        "    fact, even the `Conv2d` and `Linear` layer classes inherit from\n",
        "    `torch.nn.Module`.\n",
        "-   A model will have an `__init__()` function, where it instantiates\n",
        "    its layers, and loads any data artifacts it might need (e.g., an NLP\n",
        "    model might load a vocabulary).\n",
        "-   A model will have a `forward()` function. This is where the actual\n",
        "    computation happens: An input is passed through the network layers\n",
        "    and various functions to generate an output.\n",
        "-   Other than that, you can build out your model class like any other\n",
        "    Python class, adding whatever properties and methods you need to\n",
        "    support your model's computation.\n",
        "\n",
        "Let's instantiate this object and run a sample input through it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXSluwBdEtpD",
        "outputId": "4a257fd3-621c-4cd7-85ad-edc44bbfdffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LeNet(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "Image batch shape:\n",
            "torch.Size([1, 1, 32, 32])\n",
            "\n",
            "Raw output:\n",
            "tensor([[ 0.0898,  0.0318,  0.1485,  0.0301, -0.0085, -0.1135, -0.0296,  0.0164,\n",
            "          0.0039,  0.0616]], grad_fn=<AddmmBackward0>)\n",
            "torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "net = LeNet()\n",
        "print(net)                         # what does the object tell us about itself?\n",
        "\n",
        "input = torch.rand(1, 1, 32, 32)   # stand-in for a 32x32 black & white image\n",
        "print('\\nImage batch shape:')\n",
        "print(input.shape)\n",
        "\n",
        "output = net(input)                # we don't call forward() directly\n",
        "print('\\nRaw output:')\n",
        "print(output)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sWn9mxCEtpE"
      },
      "source": [
        "There are a few important things happening above:\n",
        "\n",
        "First, we instantiate the `LeNet` class, and we print the `net` object.\n",
        "A subclass of `torch.nn.Module` will report the layers it has created\n",
        "and their shapes and parameters. This can provide a handy overview of a\n",
        "model if you want to get the gist of its processing.\n",
        "\n",
        "Below that, we create a dummy input representing a 32x32 image with 1\n",
        "color channel. Normally, you would load an image tile and convert it to\n",
        "a tensor of this shape.\n",
        "\n",
        "You may have noticed an extra dimension to our tensor - the *batch\n",
        "dimension.* PyTorch models assume they are working on *batches* of data\n",
        "- for example, a batch of 16 of our image tiles would have the shape\n",
        "`(16, 1, 32, 32)`. Since we're only using one image, we create a batch\n",
        "of 1 with shape `(1, 1, 32, 32)`.\n",
        "\n",
        "We ask the model for an inference by calling it like a function:\n",
        "`net(input)`. The output of this call represents the model's confidence\n",
        "that the input represents a particular digit. (Since this instance of\n",
        "the model hasn't learned anything yet, we shouldn't expect to see any\n",
        "signal in the output.) Looking at the shape of `output`, we can see that\n",
        "it also has a batch dimension, the size of which should always match the\n",
        "input batch dimension. If we had passed in an input batch of 16\n",
        "instances, `output` would have a shape of `(16, 10)`.\n",
        "\n",
        "Datasets and Dataloaders\n",
        "========================\n",
        "\n",
        "Follow along with the video beginning at\n",
        "[14:00](https://www.youtube.com/watch?v=IC0_FRiX-sw&t=840s).\n",
        "\n",
        "Below, we're going to demonstrate using one of the ready-to-download,\n",
        "open-access datasets from TorchVision, how to transform the images for\n",
        "consumption by your model, and how to use the DataLoader to feed batches\n",
        "of data to your model.\n",
        "\n",
        "The first thing we need to do is transform our incoming images into a\n",
        "PyTorch tensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HxfOPegGEtpE"
      },
      "outputs": [],
      "source": [
        "#%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9zt55knEtpE"
      },
      "source": [
        "Here, we specify two transformations for our input:\n",
        "\n",
        "-   `transforms.ToTensor()` converts images loaded by Pillow into\n",
        "    PyTorch tensors.\n",
        "-   `transforms.Normalize()` adjusts the values of the tensor so that\n",
        "    their average is zero and their standard deviation is 1.0. Most\n",
        "    activation functions have their strongest gradients around x = 0, so\n",
        "    centering our data there can speed learning. The values passed to\n",
        "    the transform are the means (first tuple) and the standard\n",
        "    deviations (second tuple) of the rgb values of the images in the\n",
        "    dataset. You can calculate these values yourself by running these\n",
        "    few lines of code:\n",
        "    `` `        from torch.utils.data import ConcatDataset        transform = transforms.Compose([transforms.ToTensor()])        trainset = torchvision.datasets.CIFAR10(root='./data', train=True,                                     download=True, transform=transform)         #stack all train images together into a tensor of shape         #(50000, 3, 32, 32)        x = torch.stack([sample[0] for sample in ConcatDataset([trainset])])         #get the mean of each channel                    mean = torch.mean(x, dim=(0,2,3)) #tensor([0.4914, 0.4822, 0.4465])        std = torch.std(x, dim=(0,2,3)) #tensor([0.2470, 0.2435, 0.2616]) ``\\`\n",
        "\n",
        "There are many more transforms available, including cropping, centering,\n",
        "rotation, and reflection.\n",
        "\n",
        "Next, we'll create an instance of the CIFAR10 dataset. This is a set of\n",
        "32x32 color image tiles representing 10 classes of objects: 6 of animals\n",
        "(bird, cat, deer, dog, frog, horse) and 4 of vehicles (airplane,\n",
        "automobile, ship, truck):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvDpnRTCEtpE",
        "outputId": "3a5070a6-1261-46d5-aa57-9fbba58ba2d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 31.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNLWaQ65EtpE"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>When you run the cell above, it may take a little time for thedataset to download.</p>\n",
        "\n",
        "</div>\n",
        "\n",
        "This is an example of creating a dataset object in PyTorch. Downloadable\n",
        "datasets (like CIFAR-10 above) are subclasses of\n",
        "`torch.utils.data.Dataset`. `Dataset` classes in PyTorch include the\n",
        "downloadable datasets in TorchVision, Torchtext, and TorchAudio, as well\n",
        "as utility dataset classes such as `torchvision.datasets.ImageFolder`,\n",
        "which will read a folder of labeled images. You can also create your own\n",
        "subclasses of `Dataset`.\n",
        "\n",
        "When we instantiate our dataset, we need to tell it a few things:\n",
        "\n",
        "-   The filesystem path to where we want the data to go.\n",
        "-   Whether or not we are using this set for training; most datasets\n",
        "    will be split into training and test subsets.\n",
        "-   Whether we would like to download the dataset if we haven't already.\n",
        "-   The transformations we want to apply to the data.\n",
        "\n",
        "Once your dataset is ready, you can give it to the `DataLoader`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1RIaVGAOEtpE"
      },
      "outputs": [],
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmSRLTo_EtpE"
      },
      "source": [
        "A `Dataset` subclass wraps access to the data, and is specialized to the\n",
        "type of data it's serving. The `DataLoader` knows *nothing* about the\n",
        "data, but organizes the input tensors served by the `Dataset` into\n",
        "batches with the parameters you specify.\n",
        "\n",
        "In the example above, we've asked a `DataLoader` to give us batches of 4\n",
        "images from `trainset`, randomizing their order (`shuffle=True`), and we\n",
        "told it to spin up two workers to load data from disk.\n",
        "\n",
        "It's good practice to visualize the batches your `DataLoader` serves:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "sCiCaqfiEtpE",
        "outputId": "58b57ddd-1fca-41b8-8513-db9f07e9b5b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.5632443].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ship   car horse  ship\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pvoMSvXEtpE"
      },
      "source": [
        "Running the above cell should show you a strip of four images, and the\n",
        "correct label for each.\n",
        "\n",
        "Training Your PyTorch Model\n",
        "===========================\n",
        "\n",
        "Follow along with the video beginning at\n",
        "[17:10](https://www.youtube.com/watch?v=IC0_FRiX-sw&t=1030s).\n",
        "\n",
        "Let's put all the pieces together, and train a model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IxK23yN-EtpE"
      },
      "outputs": [],
      "source": [
        "#%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4xH2is5EtpF"
      },
      "source": [
        "First, we'll need training and test datasets. If you haven't already,\n",
        "run the cell below to make sure the dataset is downloaded. (It may take\n",
        "a minute.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgW9LNEFEtpF",
        "outputId": "0d873c3e-da75-4c4c-f175-712da2c4a332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WquM7xoyEtpF"
      },
      "source": [
        "We'll run our check on the output from `DataLoader`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "VUFW8J-gEtpF",
        "outputId": "de67644f-9db2-4690-faed-5a2a6e8fe41d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  cat   cat  deer  frog\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6RwJI6eEtpF"
      },
      "source": [
        "This is the model we'll train. If it looks familiar, that's because it's\n",
        "a variant of LeNet - discussed earlier in this video - adapted for\n",
        "3-color images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "D6lo6CJzEtpF"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FxBnmz7EtpF"
      },
      "source": [
        "The last ingredients we need are a loss function and an optimizer:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SaFiNZPxEtpF"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQX15CUVEtpF"
      },
      "source": [
        "The loss function, as discussed earlier in this video, is a measure of\n",
        "how far from our ideal output the model's prediction was. Cross-entropy\n",
        "loss is a typical loss function for classification models like ours.\n",
        "\n",
        "The **optimizer** is what drives the learning. Here we have created an\n",
        "optimizer that implements *stochastic gradient descent,* one of the more\n",
        "straightforward optimization algorithms. Besides parameters of the\n",
        "algorithm, like the learning rate (`lr`) and momentum, we also pass in\n",
        "`net.parameters()`, which is a collection of all the learning weights in\n",
        "the model - which is what the optimizer adjusts.\n",
        "\n",
        "Finally, all of this is assembled into the training loop. Go ahead and\n",
        "run this cell, as it will likely take a few minutes to execute:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-dh0jwuEtpF",
        "outputId": "bfbc69da-3d8d-476d-d3c8-28d3d59d50a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.195\n",
            "[1,  4000] loss: 1.876\n",
            "[1,  6000] loss: 1.654\n",
            "[1,  8000] loss: 1.576\n",
            "[1, 10000] loss: 1.517\n",
            "[1, 12000] loss: 1.463\n",
            "[2,  2000] loss: 1.416\n",
            "[2,  4000] loss: 1.376\n",
            "[2,  6000] loss: 1.338\n",
            "[2,  8000] loss: 1.331\n",
            "[2, 10000] loss: 1.317\n",
            "[2, 12000] loss: 1.273\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oaa-yZEEtpK"
      },
      "source": [
        "Here, we are doing only **2 training epochs** (line 1) - that is, two\n",
        "passes over the training dataset. Each pass has an inner loop that\n",
        "**iterates over the training data** (line 4), serving batches of\n",
        "transformed input images and their correct labels.\n",
        "\n",
        "**Zeroing the gradients** (line 9) is an important step. Gradients are\n",
        "accumulated over a batch; if we do not reset them for every batch, they\n",
        "will keep accumulating, which will provide incorrect gradient values,\n",
        "making learning impossible.\n",
        "\n",
        "In line 12, we **ask the model for its predictions** on this batch. In\n",
        "the following line (13), we compute the loss - the difference between\n",
        "`outputs` (the model prediction) and `labels` (the correct output).\n",
        "\n",
        "In line 14, we do the `backward()` pass, and calculate the gradients\n",
        "that will direct the learning.\n",
        "\n",
        "In line 15, the optimizer performs one learning step - it uses the\n",
        "gradients from the `backward()` call to nudge the learning weights in\n",
        "the direction it thinks will reduce the loss.\n",
        "\n",
        "The remainder of the loop does some light reporting on the epoch number,\n",
        "how many training instances have been completed, and what the collected\n",
        "loss is over the training loop.\n",
        "\n",
        "**When you run the cell above,** you should see something like this:\n",
        "\n",
        "``` {.sh}\n",
        "[1,  2000] loss: 2.235\n",
        "[1,  4000] loss: 1.940\n",
        "[1,  6000] loss: 1.713\n",
        "[1,  8000] loss: 1.573\n",
        "[1, 10000] loss: 1.507\n",
        "[1, 12000] loss: 1.442\n",
        "[2,  2000] loss: 1.378\n",
        "[2,  4000] loss: 1.364\n",
        "[2,  6000] loss: 1.349\n",
        "[2,  8000] loss: 1.319\n",
        "[2, 10000] loss: 1.284\n",
        "[2, 12000] loss: 1.267\n",
        "Finished Training\n",
        "```\n",
        "\n",
        "Note that the loss is monotonically descending, indicating that our\n",
        "model is continuing to improve its performance on the training dataset.\n",
        "\n",
        "As a final step, we should check that the model is actually doing\n",
        "*general* learning, and not simply \"memorizing\" the dataset. This is\n",
        "called **overfitting,** and usually indicates that the dataset is too\n",
        "small (not enough examples for general learning), or that the model has\n",
        "more learning parameters than it needs to correctly model the dataset.\n",
        "\n",
        "This is the reason datasets are split into training and test subsets -to\n",
        "test the generality of the model, we ask it to make predictions on data\n",
        "it hasn't trained on:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_FdcZwrEtpL",
        "outputId": "01db7ae2-40a9-4406-c629-0e5ebd25c428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 55 %\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2iAowgNEtpL"
      },
      "source": [
        "If you followed along, you should see that the model is roughly 50%\n",
        "accurate at this point. That's not exactly state-of-the-art, but it's\n",
        "far better than the 10% accuracy we'd expect from a random output. This\n",
        "demonstrates that some general learning did happen in the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_Uyzcn7gSgIM"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZjimGIsSgIN"
      },
      "source": [
        "[Introduction](introyt1_tutorial.html) \\|\\| **Tensors** \\|\\|\n",
        "[Autograd](autogradyt_tutorial.html) \\|\\| [Building\n",
        "Models](modelsyt_tutorial.html) \\|\\| [TensorBoard\n",
        "Support](tensorboardyt_tutorial.html) \\|\\| [Training\n",
        "Models](trainingyt.html) \\|\\| [Model Understanding](captumyt.html)\n",
        "\n",
        "Introduction to PyTorch Tensors\n",
        "===============================\n",
        "\n",
        "Follow along with the video below or on\n",
        "[youtube](https://www.youtube.com/watch?v=r7QDUPb2dCM).\n",
        "\n",
        "``` {.python .jupyter-code-cell}\n",
        "from IPython.display import display, HTML\n",
        "html_code = \"\"\"\n",
        "<div style=\"margin-top:10px; margin-bottom:10px;\">\n",
        "  <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/r7QDUPb2dCM\" frameborder=\"0\" allow=\"accelerometer; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
        "</div>\n",
        "\"\"\"\n",
        "display(HTML(html_code))\n",
        "```\n",
        "\n",
        "Tensors are the central data abstraction in PyTorch. This interactive\n",
        "notebook provides an in-depth introduction to the `torch.Tensor` class.\n",
        "\n",
        "First things first, let's import the PyTorch module. We'll also add\n",
        "Python's math module to facilitate some of the examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MgSwN34USgIO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRLlEHjZSgIO"
      },
      "source": [
        "Creating Tensors\n",
        "================\n",
        "\n",
        "The simplest way to create a tensor is with the `torch.empty()` call:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNRmP58YSgIO",
        "outputId": "ad4a2272-11fe-49be-9708-2a9b5c577b94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "tensor([[1.6211e-32, 0.0000e+00, 9.2825e-33, 0.0000e+00],\n",
            "        [3.1840e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [1.2859e-32, 0.0000e+00, 1.6664e+13, 4.5572e-41]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.empty(3, 4)\n",
        "print(type(x))\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hrt_6x5SgIO"
      },
      "source": [
        "Let's upack what we just did:\n",
        "\n",
        "-   We created a tensor using one of the numerous factory methods\n",
        "    attached to the `torch` module.\n",
        "-   The tensor itself is 2-dimensional, having 3 rows and 4 columns.\n",
        "-   The type of the object returned is `torch.Tensor`, which is an alias\n",
        "    for `torch.FloatTensor`; by default, PyTorch tensors are populated\n",
        "    with 32-bit floating point numbers. (More on data types below.)\n",
        "-   You will probably see some random-looking values when printing your\n",
        "    tensor. The `torch.empty()` call allocates memory for the tensor,\n",
        "    but does not initialize it with any values - so what you're seeing\n",
        "    is whatever was in memory at the time of allocation.\n",
        "\n",
        "A brief note about tensors and their number of dimensions, and\n",
        "terminology:\n",
        "\n",
        "-   You will sometimes see a 1-dimensional tensor called a *vector.*\n",
        "-   Likewise, a 2-dimensional tensor is often referred to as a *matrix.*\n",
        "-   Anything with more than two dimensions is generally just called a\n",
        "    tensor.\n",
        "\n",
        "More often than not, you'll want to initialize your tensor with some\n",
        "value. Common cases are all zeros, all ones, or random values, and the\n",
        "`torch` module provides factory methods for all of these:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAaPjEcCSgIP",
        "outputId": "7f38a02f-72be-4d2b-d592-af3f7b7deb74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[0.3126, 0.3791, 0.3087],\n",
            "        [0.0736, 0.4216, 0.0691]])\n"
          ]
        }
      ],
      "source": [
        "zeros = torch.zeros(2, 3)\n",
        "print(zeros)\n",
        "\n",
        "ones = torch.ones(2, 3)\n",
        "print(ones)\n",
        "\n",
        "torch.manual_seed(1729)\n",
        "random = torch.rand(2, 3)\n",
        "print(random)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ5Eqkg_SgIP"
      },
      "source": [
        "The fctory methods all do just what you'd expect - we have a tensor full\n",
        "of zeros, another full of ones, and another with random values between 0\n",
        "and 1.\n",
        "\n",
        "Random Tensors and Seeding\n",
        "==========================\n",
        "\n",
        "Speaking of the random tensor, did you notice the call to\n",
        "`torch.manual_seed()` immediately preceding it? Initializing tensors,\n",
        "such as a model's learning weights, with random values is common but\n",
        "there are times - especially in research settings - where you'll want\n",
        "some assurance of the reproducibility of your results. Manually setting\n",
        "your random number generator's seed is the way to do this. Let's look\n",
        "more closely:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X--wBfhlSgIP",
        "outputId": "0e947a59-b308-4311-d4d1-de54a68ea146"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3126, 0.3791, 0.3087],\n",
            "        [0.0736, 0.4216, 0.0691]])\n",
            "tensor([[0.2332, 0.4047, 0.2162],\n",
            "        [0.9927, 0.4128, 0.5938]])\n",
            "tensor([[0.3126, 0.3791, 0.3087],\n",
            "        [0.0736, 0.4216, 0.0691]])\n",
            "tensor([[0.2332, 0.4047, 0.2162],\n",
            "        [0.9927, 0.4128, 0.5938]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1729)\n",
        "random1 = torch.rand(2, 3)\n",
        "print(random1)\n",
        "\n",
        "random2 = torch.rand(2, 3)\n",
        "print(random2)\n",
        "\n",
        "torch.manual_seed(1729)\n",
        "random3 = torch.rand(2, 3)\n",
        "print(random3)\n",
        "\n",
        "random4 = torch.rand(2, 3)\n",
        "print(random4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53c8WKIKSgIP"
      },
      "source": [
        "What you should see above is that `random1` and `random3` carry\n",
        "identical values, as do `random2` and `random4`. Manually setting the\n",
        "RNG's seed resets it, so that identical computations depending on random\n",
        "number should, in most settings, provide identical results.\n",
        "\n",
        "For more information, see the [PyTorch documentation on\n",
        "reproducibility](https://pytorch.org/docs/stable/notes/randomness.html).\n",
        "\n",
        "Tensor Shapes\n",
        "=============\n",
        "\n",
        "Often, when you're performing operations on two or more tensors, they\n",
        "will need to be of the same *shape* - that is, having the same number of\n",
        "dimensions and the same number of cells in each dimension. For that, we\n",
        "have the `torch.*_like()` methods:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by7dgBbaSgIP",
        "outputId": "d8a88b2f-9983-4054-e107-b8706bb573a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2, 3])\n",
            "tensor([[[6.7696e-33, 0.0000e+00, 0.0000e+00],\n",
            "         [1.4013e-45, 8.9683e-44, 0.0000e+00]],\n",
            "\n",
            "        [[1.1210e-43, 0.0000e+00, 1.4441e-33],\n",
            "         [0.0000e+00, 5.2360e-33, 0.0000e+00]]])\n",
            "torch.Size([2, 2, 3])\n",
            "tensor([[[-4.3854e-17,  4.5572e-41,  1.8545e-33],\n",
            "         [ 0.0000e+00,  4.4842e-44,  0.0000e+00]],\n",
            "\n",
            "        [[ 1.5695e-43,  0.0000e+00,  1.8597e-33],\n",
            "         [ 0.0000e+00,  1.4013e-45,  0.0000e+00]]])\n",
            "torch.Size([2, 2, 3])\n",
            "tensor([[[0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.]]])\n",
            "torch.Size([2, 2, 3])\n",
            "tensor([[[1., 1., 1.],\n",
            "         [1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1.],\n",
            "         [1., 1., 1.]]])\n",
            "torch.Size([2, 2, 3])\n",
            "tensor([[[0.6128, 0.1519, 0.0453],\n",
            "         [0.5035, 0.9978, 0.3884]],\n",
            "\n",
            "        [[0.6929, 0.1703, 0.1384],\n",
            "         [0.4759, 0.7481, 0.0361]]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.empty(2, 2, 3)\n",
        "print(x.shape)\n",
        "print(x)\n",
        "\n",
        "empty_like_x = torch.empty_like(x)\n",
        "print(empty_like_x.shape)\n",
        "print(empty_like_x)\n",
        "\n",
        "zeros_like_x = torch.zeros_like(x)\n",
        "print(zeros_like_x.shape)\n",
        "print(zeros_like_x)\n",
        "\n",
        "ones_like_x = torch.ones_like(x)\n",
        "print(ones_like_x.shape)\n",
        "print(ones_like_x)\n",
        "\n",
        "rand_like_x = torch.rand_like(x)\n",
        "print(rand_like_x.shape)\n",
        "print(rand_like_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrytyNT7SgIP"
      },
      "source": [
        "The first new thing in the code cell above is the use of the `.shape`\n",
        "property on a tensor. This property contains a list of the extent of\n",
        "each dimension of a tensor - in our case, `x` is a three-dimensional\n",
        "tensor with shape 2 x 2 x 3.\n",
        "\n",
        "Below that, we call the `.empty_like()`, `.zeros_like()`,\n",
        "`.ones_like()`, and `.rand_like()` methods. Using the `.shape` property,\n",
        "we can verify that each of these methods returns a tensor of identical\n",
        "dimensionality and extent.\n",
        "\n",
        "The last way to create a tensor that will cover is to specify its data\n",
        "directly from a PyTorch collection:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3RS3nUASgIP",
        "outputId": "2e0ff945-9cec-4ac5-c62d-c4f9fbcf642f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3.1416, 2.7183],\n",
            "        [1.6180, 0.0073]])\n",
            "tensor([ 2,  3,  5,  7, 11, 13, 17, 19])\n",
            "tensor([[2, 4, 6],\n",
            "        [3, 6, 9]])\n"
          ]
        }
      ],
      "source": [
        "some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])\n",
        "print(some_constants)\n",
        "\n",
        "some_integers = torch.tensor((2, 3, 5, 5, 11, 13, 17, 19))\n",
        "print(some_integers)\n",
        "\n",
        "more_integers = torch.tensor(((2, 4, 6), [3, 6, 9]))\n",
        "print(more_integers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HC54U1USgIP"
      },
      "source": [
        "Using `torch.tensor()` is the most straightforward way to create a\n",
        "tensor if you already have data in a Python tuple or list. As shown\n",
        "above, nesting the collections will result in a multi-dimensional\n",
        "tensor.\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p><code>torch.tensor()</code> creates a copy of the data.</p>\n",
        "\n",
        "</div>\n",
        "\n",
        "Tensor Data Types\n",
        "=================\n",
        "\n",
        "Setting the datatype of a tensor is possible a couple of ways:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkwBbY3ASgIQ",
        "outputId": "92532377-9881-4902-b5c5-4493284e9223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 1, 1],\n",
            "        [1, 1, 1]], dtype=torch.int16)\n",
            "tensor([[ 0.9956,  1.4148,  5.8364],\n",
            "        [11.2406, 11.2083, 11.6692]], dtype=torch.float64)\n",
            "tensor([[ 0,  1,  5],\n",
            "        [11, 11, 11]], dtype=torch.int32)\n"
          ]
        }
      ],
      "source": [
        "a = torch.ones((2, 3), dtype=torch.int16)\n",
        "print(a)\n",
        "\n",
        "b = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
        "print(b)\n",
        "\n",
        "c = b.to(torch.int32)\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSS4jmHISgIQ"
      },
      "source": [
        "The simplest way to set the underlying data type of a tensor is with an\n",
        "optional argument at creation time. In the first line of the cell above,\n",
        "we set `dtype=torch.int16` for the tensor `a`. When we print `a`, we can\n",
        "see that it's full of `1` rather than `1.` - Python's subtle cue that\n",
        "this is an integer type rather than floating point.\n",
        "\n",
        "Another thing to notice about printing `a` is that, unlike when we left\n",
        "`dtype` as the default (32-bit floating point), printing the tensor also\n",
        "specifies its `dtype`.\n",
        "\n",
        "You may have also spotted that we went from specifying the tensor's\n",
        "shape as a series of integer arguments, to grouping those arguments in a\n",
        "tuple. This is not strictly necessary - PyTorch will take a series of\n",
        "initial, unlabeled integer arguments as a tensor shape - but when adding\n",
        "the optional arguments, it can make your intent more readable.\n",
        "\n",
        "The other way to set the datatype is with the `.to()` method. In the\n",
        "cell above, we create a random floating point tensor `b` in the usual\n",
        "way. Following that, we create `c` by converting `b` to a 32-bit integer\n",
        "with the `.to()` method. Note that `c` contains all the same values as\n",
        "`b`, but truncated to integers.\n",
        "\n",
        "For more information, see the [data types\n",
        "documentation](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype).\n",
        "\n",
        "Math & Logic with PyTorch Tensors\n",
        "=================================\n",
        "\n",
        "Now that you know some of the ways to create a tensor... what can you do\n",
        "with them?\n",
        "\n",
        "Let's look at basic arithmetic first, and how tensors interact with\n",
        "simple scalars:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4eLkQXrSgIQ",
        "outputId": "842d0fc1-7e74-4261-8398-9f1b1e2dea2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[2., 2.],\n",
            "        [2., 2.]])\n",
            "tensor([[3., 3.],\n",
            "        [3., 3.]])\n",
            "tensor([[4., 4.],\n",
            "        [4., 4.]])\n",
            "tensor([[1.4142, 1.4142],\n",
            "        [1.4142, 1.4142]])\n"
          ]
        }
      ],
      "source": [
        "ones = torch.zeros(2, 2) + 1\n",
        "twos = torch.ones(2, 2) * 2\n",
        "threes = (torch.ones(2, 2) * 7 - 1) / 2\n",
        "fours = twos ** 2\n",
        "sqrt2s = twos ** 0.5\n",
        "\n",
        "print(ones)\n",
        "print(twos)\n",
        "print(threes)\n",
        "print(fours)\n",
        "print(sqrt2s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07eO2MYeSgIQ"
      },
      "source": [
        "As you can see above, arithmetic operations between tensors and scalars,\n",
        "such as addition, subtraction, multiplication, division, and\n",
        "exponentiation are distributed over every element of the tensor. Because\n",
        "the output of such an operation will be a tensor, you can chain them\n",
        "together with the usual operator precedence rules, as in the line where\n",
        "we create `threes`.\n",
        "\n",
        "Similar operations between two tensors also behave like you'd\n",
        "intuitively expect:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97L07PTQSgIQ",
        "outputId": "5b9e3cca-5207-43dc-e767-dafdbe68c932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.,  4.],\n",
            "        [ 8., 16.]])\n",
            "tensor([[5., 5.],\n",
            "        [5., 5.]])\n",
            "tensor([[12., 12.],\n",
            "        [12., 12.]])\n"
          ]
        }
      ],
      "source": [
        "powers2 = twos ** torch.tensor([[1, 2], [3, 4]])\n",
        "print(powers2)\n",
        "\n",
        "fives = ones + fours\n",
        "print(fives)\n",
        "\n",
        "dozens = threes * fours\n",
        "print(dozens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwqKTzGTSgIQ"
      },
      "source": [
        "It's important to note here that all of the tensors in the previous code\n",
        "cell were of identical shape. What happens when we try to perform a\n",
        "binary operation on tensors if dissimilar shape?\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>The following cell throws a run-time error. This is intentional.<pre><code>a = torch.rand(2, 3)\n",
        "b = torch.rand(3, 2)</p>\n",
        "<p>print(a * b)</code></pre></p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQESD1VASgIQ"
      },
      "source": [
        "In the general case, you cannot operate on tensors of different shape\n",
        "this way, even in a case like the cell above, where the tensors have an\n",
        "identical number of elements.\n",
        "\n",
        "In Brief: Tensor Broadcasting\n",
        "=============================\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>If you are familiar with broadcasting semantics in NumPyndarrays, you’ll find the same rules apply here.</p>\n",
        "\n",
        "</div>\n",
        "\n",
        "The exception to the same-shapes rule is *tensor broadcasting.* Here's\n",
        "an example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKkt9tDzSgIR",
        "outputId": "67acacde-70c1-4b93-c1c4-957b0bfa8ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6146, 0.5999, 0.5013, 0.9397],\n",
            "        [0.8656, 0.5207, 0.6865, 0.3614]])\n",
            "tensor([[1.2291, 1.1998, 1.0026, 1.8793],\n",
            "        [1.7312, 1.0413, 1.3730, 0.7228]])\n"
          ]
        }
      ],
      "source": [
        "rand = torch.rand(2, 4)\n",
        "doubled = rand * (torch.ones(1, 4) * 2)\n",
        "\n",
        "print(rand)\n",
        "print(doubled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2KLTBbKSgIR"
      },
      "source": [
        "What's the trick here? How is it we got to multiply a 2x4 tensor by a\n",
        "1x4 tensor?\n",
        "\n",
        "Broadcasting is a way to perform an operation between tensors that have\n",
        "similarities in their shapes. In the example above, the one-row,\n",
        "four-column tensor is multiplied by *both rows* of the two-row,\n",
        "four-column tensor.\n",
        "\n",
        "This is an important operation in Deep Learning. The common example is\n",
        "multiplying a tensor of learning weights by a *batch* of input tensors,\n",
        "applying the operation to each instance in the batch separately, and\n",
        "returning a tensor of identical shape - just like our (2, 4) \\* (1, 4)\n",
        "example above returned a tensor of shape (2, 4).\n",
        "\n",
        "The rules for broadcasting are:\n",
        "\n",
        "-   Each tensor must have at least one dimension - no empty tensors.\n",
        "-   Comparing the dimension sizes of the two tensors, *going from last\n",
        "    to first:*\n",
        "    -   Each dimension must be equal, *or*\n",
        "    -   One of the dimensions must be of size 1, *or*\n",
        "    -   The dimension does not exist in one of the tensors\n",
        "\n",
        "Tensors of identical shape, of course, are trivially \"broadcastable\", as\n",
        "you saw earlier.\n",
        "\n",
        "Here are some examples of situations that honor the above rules and\n",
        "allow broadcasting:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-vGALw6SgIR",
        "outputId": "02e0d8b8-4d8c-4382-8cbb-4b2ed7a70056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.6493, 0.2633],\n",
            "         [0.4762, 0.0548],\n",
            "         [0.2024, 0.5731]],\n",
            "\n",
            "        [[0.6493, 0.2633],\n",
            "         [0.4762, 0.0548],\n",
            "         [0.2024, 0.5731]],\n",
            "\n",
            "        [[0.6493, 0.2633],\n",
            "         [0.4762, 0.0548],\n",
            "         [0.2024, 0.5731]],\n",
            "\n",
            "        [[0.6493, 0.2633],\n",
            "         [0.4762, 0.0548],\n",
            "         [0.2024, 0.5731]]])\n",
            "tensor([[[0.7191, 0.7191],\n",
            "         [0.4067, 0.4067],\n",
            "         [0.7301, 0.7301]],\n",
            "\n",
            "        [[0.7191, 0.7191],\n",
            "         [0.4067, 0.4067],\n",
            "         [0.7301, 0.7301]],\n",
            "\n",
            "        [[0.7191, 0.7191],\n",
            "         [0.4067, 0.4067],\n",
            "         [0.7301, 0.7301]],\n",
            "\n",
            "        [[0.7191, 0.7191],\n",
            "         [0.4067, 0.4067],\n",
            "         [0.7301, 0.7301]]])\n",
            "tensor([[[0.6276, 0.7357],\n",
            "         [0.6276, 0.7357],\n",
            "         [0.6276, 0.7357]],\n",
            "\n",
            "        [[0.6276, 0.7357],\n",
            "         [0.6276, 0.7357],\n",
            "         [0.6276, 0.7357]],\n",
            "\n",
            "        [[0.6276, 0.7357],\n",
            "         [0.6276, 0.7357],\n",
            "         [0.6276, 0.7357]],\n",
            "\n",
            "        [[0.6276, 0.7357],\n",
            "         [0.6276, 0.7357],\n",
            "         [0.6276, 0.7357]]])\n"
          ]
        }
      ],
      "source": [
        "a =     torch.ones(4, 3, 2)\n",
        "\n",
        "b = a * torch.rand(   3, 2) # 3rd & 2nd dims identical to a, dim 1 absent\n",
        "print(b)\n",
        "\n",
        "c = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\n",
        "print(c)\n",
        "\n",
        "d = a * torch.rand(   1, 2) # 3rd dim identical to a, 2nd dim = 1\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5O0qI9sSgIR"
      },
      "source": [
        "Look closely at the values of each tensor above:\n",
        "\n",
        "-   The multiplication operation that created `b` was broadcast over\n",
        "    every \"layer\" of `a`.\n",
        "-   For `c`, the operation was broadcast over every layer and row of\n",
        "    `a` - every 3-element column is identical.\n",
        "-   For `d`, we switched it around - now every *row* is identical,\n",
        "    across layers and columns.\n",
        "\n",
        "For more information on broadcasting, see the [PyTorch\n",
        "documentation](https://pytorch.org/docs/stable/notes/broadcasting.html)\n",
        "on the topic.\n",
        "\n",
        "Here are some examples of attempts at broadcasting that will fail:\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>The following cell throws a run-time error. This is intentional.<pre><code>a =     torch.ones(4, 3, 2)</p>\n",
        "<p>b = a * torch.rand(4, 3)    # dimensions must match last-to-first</p>\n",
        "<p>c = a * torch.rand(   2, 3) # both 3rd &amp; 2nd dims different</p>\n",
        "<p>d = a * torch.rand((0, ))   # can&#x27;t broadcast with an empty tensor</code></pre></p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMWrCM-vSgIR"
      },
      "source": [
        "More Math with Tensors\n",
        "======================\n",
        "\n",
        "PyTorch tensors have over three hundred operations that can be performed\n",
        "on them.\n",
        "\n",
        "Here is a small sample from some of the major categories of operations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjF2J-vsSgIR",
        "outputId": "da28788d-cd9c-48f8-cd4b-0f647b80e2c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common functions:\n",
            "tensor([[0.9238, 0.5724, 0.0791, 0.2629],\n",
            "        [0.1986, 0.4439, 0.6434, 0.4776]])\n",
            "tensor([[-0., -0., 1., -0.],\n",
            "        [-0., 1., 1., -0.]])\n",
            "tensor([[-1., -1.,  0., -1.],\n",
            "        [-1.,  0.,  0., -1.]])\n",
            "tensor([[-0.5000, -0.5000,  0.0791, -0.2629],\n",
            "        [-0.1986,  0.4439,  0.5000, -0.4776]])\n",
            "\n",
            "Sine and arcsine:\n",
            "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
            "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
            "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n",
            "\n",
            "Bitwise XOR:\n",
            "tensor([3, 2, 1])\n",
            "\n",
            "Broadcasted, element-wise equality comparison:\n",
            "tensor([[ True, False],\n",
            "        [False, False]])\n",
            "\n",
            "Reduction ops:\n",
            "tensor(4.)\n",
            "4.0\n",
            "tensor(2.5000)\n",
            "tensor(1.2910)\n",
            "tensor(24.)\n",
            "tensor([1, 2])\n",
            "\n",
            "Vectors & Matrices:\n",
            "tensor([ 0.,  0., -1.])\n",
            "tensor([[0.7375, 0.8328],\n",
            "        [0.8444, 0.2941]])\n",
            "tensor([[2.2125, 2.4985],\n",
            "        [2.5332, 0.8822]])\n",
            "torch.return_types.linalg_svd(\n",
            "U=tensor([[-0.7889, -0.6145],\n",
            "        [-0.6145,  0.7889]]),\n",
            "S=tensor([4.1498, 1.0548]),\n",
            "Vh=tensor([[-0.7957, -0.6056],\n",
            "        [ 0.6056, -0.7957]]))\n"
          ]
        }
      ],
      "source": [
        "# common functions\n",
        "a = torch.rand(2, 4) * 2 - 1\n",
        "print('Common functions:')\n",
        "print(torch.abs(a))\n",
        "print(torch.ceil(a))\n",
        "print(torch.floor(a))\n",
        "print(torch.clamp(a, -0.5, 0.5))\n",
        "\n",
        "# trigonometric functions and their inverses\n",
        "angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
        "sines = torch.sin(angles)\n",
        "inverses = torch.asin(sines)\n",
        "print('\\nSine and arcsine:')\n",
        "print(angles)\n",
        "print(sines)\n",
        "print(inverses)\n",
        "\n",
        "# bitwise operations\n",
        "print('\\nBitwise XOR:')\n",
        "b = torch.tensor([1, 5, 11])\n",
        "c = torch.tensor([2, 7, 10])\n",
        "print(torch.bitwise_xor(b, c))\n",
        "\n",
        "# comparisons:\n",
        "print('\\nBroadcasted, element-wise equality comparison:')\n",
        "d = torch.tensor([[1., 2.], [3., 4.]])\n",
        "e = torch.ones(1, 2)  # many comparison ops support broadcasting!\n",
        "print(torch.eq(d, e)) # returns a tensor of type bool\n",
        "\n",
        "# reductions:\n",
        "print('\\nReduction ops:')\n",
        "print(torch.max(d))        # returns a single-element tensor\n",
        "print(torch.max(d).item()) # extracts the value from the returned tensor\n",
        "print(torch.mean(d))       # average\n",
        "print(torch.std(d))        # standard deviation\n",
        "print(torch.prod(d))       # product of all numbers\n",
        "print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # filter unique elements\n",
        "\n",
        "# vector and linear algebra operations\n",
        "v1 = torch.tensor([1., 0., 0.])         # x unit vector\n",
        "v2 = torch.tensor([0., 1., 0.])         # y unit vector\n",
        "m1 = torch.rand(2, 2)                   # random matrix\n",
        "m2 = torch.tensor([[3., 0.], [0., 3.]]) # three times identity matrix\n",
        "\n",
        "print('\\nVectors & Matrices:')\n",
        "print(torch.linalg.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1)\n",
        "print(m1)\n",
        "m3 = torch.linalg.matmul(m1, m2)\n",
        "print(m3)                  # 3 times m1\n",
        "print(torch.linalg.svd(m3))       # singular value decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZvlC-QDSgIR"
      },
      "source": [
        "This is a small sample of operations. For more details and the full\n",
        "inventory of math functions, have a look at the\n",
        "[documentation](https://pytorch.org/docs/stable/torch.html#math-operations).\n",
        "For more details and the full inventory of linear algebra operations,\n",
        "have a look at this\n",
        "[documentation](https://pytorch.org/docs/stable/linalg.html).\n",
        "\n",
        "Altering Tensors in Place\n",
        "=========================\n",
        "\n",
        "Most binary operations on tensors will return a third, new tensor. When\n",
        "we say `c = a * b` (where `a` and `b` are tensors), the new tensor `c`\n",
        "will occupy a region of memory distinct from the other tensors.\n",
        "\n",
        "There are times, though, that you may wish to alter a tensor in place\n",
        "-for example, if you're doing an element-wise computation where you can\n",
        "discard intermediate values. For this, most of the math functions have a\n",
        "version with an appended underscore (`_`) that will alter a tensor in\n",
        "place.\n",
        "\n",
        "For example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DthquPyoSgIR",
        "outputId": "1e8a2790-867f-4337-9b9d-bb60350c9e5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a:\n",
            "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
            "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
            "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
            "\n",
            "b:\n",
            "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
            "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
            "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n"
          ]
        }
      ],
      "source": [
        "a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
        "print('a:')\n",
        "print(a)\n",
        "print(torch.sin(a))   # this operation creates a new tensor in memory\n",
        "print(a)              # a has not changed\n",
        "\n",
        "b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
        "print('\\nb:')\n",
        "print(b)\n",
        "print(torch.sin_(b))  # note the underscore\n",
        "print(b)              # b has changed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udtan4_rSgIR"
      },
      "source": [
        "For arithmetic operations, there are functions that behave similarly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grgU9wU1SgIR",
        "outputId": "32b8dd9e-cbb6-40f0-b975-531afa2f4440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[0.3788, 0.4567],\n",
            "        [0.0649, 0.6677]])\n",
            "\n",
            "After adding:\n",
            "tensor([[1.3788, 1.4567],\n",
            "        [1.0649, 1.6677]])\n",
            "tensor([[1.3788, 1.4567],\n",
            "        [1.0649, 1.6677]])\n",
            "tensor([[0.3788, 0.4567],\n",
            "        [0.0649, 0.6677]])\n",
            "\n",
            "After multiplying\n",
            "tensor([[0.1435, 0.2086],\n",
            "        [0.0042, 0.4459]])\n",
            "tensor([[0.1435, 0.2086],\n",
            "        [0.0042, 0.4459]])\n"
          ]
        }
      ],
      "source": [
        "a = torch.ones(2, 2)\n",
        "b = torch.rand(2, 2)\n",
        "\n",
        "print('Before:')\n",
        "print(a)\n",
        "print(b)\n",
        "print('\\nAfter adding:')\n",
        "print(a.add_(b))\n",
        "print(a)\n",
        "print(b)\n",
        "print('\\nAfter multiplying')\n",
        "print(b\n",
        "      .mul_(b))\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZiu1TU-SgIR"
      },
      "source": [
        "Note that these in-place arithmetic functions are methods on the\n",
        "`torch.Tensor` object, not attached to the `torch` module like many\n",
        "other functions (e.g., `torch.sin()`). As you can see from `a.add_(b)`,\n",
        "*the calling tensor is the one that gets changed in place.*\n",
        "\n",
        "There is another option for placing the result of a computation in an\n",
        "existing, allocated tensor. Many of the methods and functions we've seen\n",
        "so far - including creation methods! - have an `out` argument that lets\n",
        "you specify a tensor to receive the output. If the `out` tensor is the\n",
        "correct shape and `dtype`, this can happen without a new memory\n",
        "allocation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OViD88zFSgIS",
        "outputId": "8a9752bb-e800-41e6-9dc5-3a63337b65f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.]])\n",
            "tensor([[0.3653, 0.8699],\n",
            "        [0.2364, 0.3604]])\n",
            "tensor([[0.0776, 0.4004],\n",
            "        [0.9877, 0.0352]])\n"
          ]
        }
      ],
      "source": [
        "a = torch.rand(2, 2)\n",
        "b = torch.rand(2, 2)\n",
        "c = torch.zeros(2, 2)\n",
        "old_id = id(c)\n",
        "\n",
        "print(c)\n",
        "d = torch.matmul(a, b, out=c)\n",
        "print(c)                # contents of c have changed\n",
        "\n",
        "assert c is d           # test c & d are same object, not just containing equal values\n",
        "assert id(c) == old_id  # make sure that our new c is the same object as the old one\n",
        "\n",
        "torch.rand(2, 2, out=c) # works for creation too!\n",
        "print(c)                # c has changed again\n",
        "assert id(c) == old_id  # still the same object!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6oo64cnSgIS"
      },
      "source": [
        "Copying Tensors\n",
        "===============\n",
        "\n",
        "As with any object in Python, assigning a tensor to a variable makes the\n",
        "variable a *label* of the tensor, and does not copy it. For example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxy2u2eaSgIS",
        "outputId": "64812f6d-a47b-4f6a-afc7-40bd0e6a58c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  1., 561.],\n",
            "        [  1.,   1.]])\n"
          ]
        }
      ],
      "source": [
        "a = torch.ones(2, 2)\n",
        "b = a\n",
        "\n",
        "a[0][1] = 561  # we change a...\n",
        "print(b)       # ...and b is also altered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7srdtLsISgIS"
      },
      "source": [
        "But what if you want a separate copy of the data to work on? The\n",
        "`clone()` method is there for you:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqy9Bpq6SgIS",
        "outputId": "74e81d98-aa2d-446c-ef18-169992430657"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[True, True],\n",
            "        [True, True]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "a = torch.ones(2, 2)\n",
        "b = a.clone()\n",
        "\n",
        "assert b is not a      # different objects in memory...\n",
        "print(torch.eq(a, b))  # ...but still with the same contents!\n",
        "\n",
        "a[0][1] = 561          # a changes...\n",
        "print(b)               # ...but b is still all ones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "repkIG2_SgIS"
      },
      "source": [
        "**There is an important thing to be aware of when using\n",
        "\\`\\`clone()\\`\\`.** If your source tensor has autograd, enabled then so\n",
        "will the clone. **This will be covered more deeply in the video on\n",
        "autograd,** but if you want the light version of the details, continue\n",
        "on.\n",
        "\n",
        "*In many cases, this will be what you want.* For example, if your model\n",
        "has multiple computation paths in its `forward()` method, and *both* the\n",
        "original tensor and its clone contribute to the model's output, then to\n",
        "enable model learning you want autograd turned on for both tensors. If\n",
        "your source tensor has autograd enabled (which it generally will if it's\n",
        "a set of learning weights or derived from a computation involving the\n",
        "weights), then you'll get the result you want.\n",
        "\n",
        "On the other hand, if you're doing a computation where *neither* the\n",
        "original tensor nor its clone need to track gradients, then as long as\n",
        "the source tensor has autograd turned off, you're good to go.\n",
        "\n",
        "*There is a third case,* though: Imagine you're performing a computation\n",
        "in your model's `forward()` function, where gradients are turned on for\n",
        "everything by default, but you want to pull out some values mid-stream\n",
        "to generate some metrics. In this case, you *don't* want the cloned copy\n",
        "of your source tensor to track gradients - performance is improved with\n",
        "autograd's history tracking turned off. For this, you can use the\n",
        "`.detach()` method on the source tensor:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_Fot7xCSgIS",
        "outputId": "abea3371-9bc3-458e-88ca-501b83236449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0905, 0.4485],\n",
            "        [0.8740, 0.2526]], requires_grad=True)\n",
            "tensor([[0.0905, 0.4485],\n",
            "        [0.8740, 0.2526]], grad_fn=<CloneBackward0>)\n",
            "tensor([[0.0905, 0.4485],\n",
            "        [0.8740, 0.2526]])\n",
            "tensor([[0.0905, 0.4485],\n",
            "        [0.8740, 0.2526]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "a = torch.rand(2, 2, requires_grad=True) # turn on autograd\n",
        "print(a)\n",
        "\n",
        "b = a.clone()\n",
        "print(b)\n",
        "\n",
        "c = a.detach().clone()\n",
        "print(c)\n",
        "\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYAED3wDSgIe"
      },
      "source": [
        "What's happening here?\n",
        "\n",
        "-   We create `a` with `requires_grad=True` turned on. **We haven't\n",
        "    covered this optional argument yet, but will during the unit on\n",
        "    autograd.**\n",
        "-   When we print `a`, it informs us that the property\n",
        "    `requires_grad=True` - this means that autograd and computation\n",
        "    history tracking are turned on.\n",
        "-   We clone `a` and label it `b`. When we print `b`, we can see that\n",
        "    it's tracking its computation history - it has inherited `a`'s\n",
        "    autograd settings, and added to the computation history.\n",
        "-   We clone `a` into `c`, but we call `detach()` first.\n",
        "-   Printing `c`, we see no computation history, and no\n",
        "    `requires_grad=True`.\n",
        "\n",
        "The `detach()` method *detaches the tensor from its computation\n",
        "history.* It says, \"do whatever comes next as if autograd was off.\" It\n",
        "does this *without* changing `a` - you can see that when we print `a`\n",
        "again at the end, it retains its `requires_grad=True` property.\n",
        "\n",
        "Moving to\n",
        "[Accelerator](https://pytorch.org/docs/stable/torch.html#accelerators)\n",
        "\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n",
        "\n",
        "One of the major advantages of PyTorch is its robust acceleration on an\n",
        "[accelerator](https://pytorch.org/docs/stable/torch.html#accelerators)\n",
        "such as CUDA, MPS, MTIA, or XPU. So far, everything we've done has been\n",
        "on CPU. How do we move to the faster hardware?\n",
        "\n",
        "First, we should check whether an accelerator is available, with the\n",
        "`is_available()` method.\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>If you do not have an accelerator, the executable cells in this section will not execute anyaccelerator-related code.</p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcr9J1mSSgIf",
        "outputId": "3e26c440-5c7f-4b0a-caab-36adc88e575a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have an accelerator!\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print('We have an accelerator!')\n",
        "else:\n",
        "    print('Sorry, CPU only.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv6X9anNSgIf"
      },
      "source": [
        "Once we've determined that one or more accelerators is available, we\n",
        "need to put our data someplace where the accelerator can see it. Your\n",
        "CPU does computation on data in your computer's RAM. Your accelerator\n",
        "has dedicated memory attached to it. Whenever you want to perform a\n",
        "computation on a device, you must move *all* the data needed for that\n",
        "computation to memory accessible by that device. (Colloquially, \"moving\n",
        "the data to memory accessible by the GPU\" is shorted to, \"moving the\n",
        "data to the GPU\".)\n",
        "\n",
        "There are multiple ways to get your data onto your target device. You\n",
        "may do it at creation time:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZzzkifWSgIf",
        "outputId": "e487c7b5-0358-4249-bb14-01f809628277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3344, 0.2640],\n",
            "        [0.2119, 0.0582]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    gpu_rand = torch.rand(2, 2, device=device)\n",
        "    print(gpu_rand)\n",
        "else:\n",
        "    print('Sorry, CPU only.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPDO2xiQSgIf"
      },
      "source": [
        "By default, new tensors are created on the CPU, so we have to specify\n",
        "when we want to create our tensor on the accelerator with the optional\n",
        "`device` argument. You can see when we print the new tensor, PyTorch\n",
        "informs us which device it's on (if it's not on CPU).\n",
        "\n",
        "You can query the number of accelerators with\n",
        "`torch.accelerator.device_count()`. If you have more than one\n",
        "accelerator, you can specify them by index, take CUDA for example:\n",
        "`device='cuda:0'`, `device='cuda:1'`, etc.\n",
        "\n",
        "As a coding practice, specifying our devices everywhere with string\n",
        "constants is pretty fragile. In an ideal world, your code would perform\n",
        "robustly whether you're on CPU or accelerator hardware. You can do this\n",
        "by creating a device handle that can be passed to your tensors instead\n",
        "of a string:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z9OeUG4SgIf",
        "outputId": "bd2af04f-8367-4acd-9f6d-b80d3eeb0431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "tensor([[0.0024, 0.6778],\n",
            "        [0.2441, 0.6812]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "my_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device: {}'.format(my_device))\n",
        "\n",
        "x = torch.rand(2, 2, device=my_device)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjX1h3pNSgIf"
      },
      "source": [
        "If you have an existing tensor living on one device, you can move it to\n",
        "another with the `to()` method. The following line of code creates a\n",
        "tensor on CPU, and moves it to whichever device handle you acquired in\n",
        "the previous cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "nUj9iwaaSgIf"
      },
      "outputs": [],
      "source": [
        "y = torch.rand(2, 2)\n",
        "y = y.to(my_device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHYyLI18SgIf"
      },
      "source": [
        "It is important to know that in order to do computation involving two or\n",
        "more tensors, *all of the tensors must be on the same device*. The\n",
        "following code will throw a runtime error, regardless of whether you\n",
        "have an accelerator device available, take CUDA for example:\n",
        "\n",
        "``` {.python}\n",
        "x = torch.rand(2, 2)\n",
        "y = torch.rand(2, 2, device='cuda')\n",
        "z = x + y  # exception will be thrown\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEGrkFNeSgIf"
      },
      "source": [
        "Manipulating Tensor Shapes\n",
        "==========================\n",
        "\n",
        "Sometimes, you'll need to change the shape of your tensor. Below, we'll\n",
        "look at a few common cases, and how to handle them.\n",
        "\n",
        "Changing the Number of Dimensions\n",
        "---------------------------------\n",
        "\n",
        "One case where you might need to change the number of dimensions is\n",
        "passing a single instance of input to your model. PyTorch models\n",
        "generally expect *batches* of input.\n",
        "\n",
        "For example, imagine having a model that works on 3 x 226 x 226 images\n",
        "-a 226-pixel square with 3 color channels. When you load and transform\n",
        "it, you'll get a tensor of shape `(3, 226, 226)`. Your model, though, is\n",
        "expecting input of shape `(N, 3, 226, 226)`, where `N` is the number of\n",
        "images in the batch. So how do you make a batch of one?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPK_iH8SSgIf",
        "outputId": "466e1ebe-c214-4388-de30-5f798cdaa5cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 226, 226])\n",
            "torch.Size([1, 3, 226, 226])\n"
          ]
        }
      ],
      "source": [
        "a = torch.rand(3, 226, 226)\n",
        "b = a.unsqueeze(0)\n",
        "\n",
        "print(a.shape)\n",
        "print(b.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEUfEYhNSgIf"
      },
      "source": [
        "The `unsqueeze()` method adds a dimension of extent 1. `unsqueeze(0)`\n",
        "adds it as a new zeroth dimension - now you have a batch of one!\n",
        "\n",
        "So if that's *un*squeezing? What do we mean by squeezing? We're taking\n",
        "advantage of the fact that any dimension of extent 1 *does not* change\n",
        "the number of elements in the tensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJC_ugJUSgIf",
        "outputId": "35a189cf-2dbd-419d-aaa5-7b5e1c11d471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[[0.2347]]]]])\n"
          ]
        }
      ],
      "source": [
        "c = torch.rand(1, 1, 1, 1, 1)\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXZWHfmlSgIg"
      },
      "source": [
        "Continuing the example above, let's say the model's output is a\n",
        "20-element vector for each input. You would then expect the output to\n",
        "have shape `(N, 20)`, where `N` is the number of instances in the input\n",
        "batch. That means that for our single-input batch, we'll get an output\n",
        "of shape `(1, 20)`.\n",
        "\n",
        "What if you want to do some *non-batched* computation with that output\n",
        "-something that's just expecting a 20-element vector?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_LNWkUjSgIg",
        "outputId": "8fd2bbfd-0525-404e-de11-c1dbf77a1d64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5])\n",
            "tensor([[0.1891, 0.3952, 0.9176, 0.8960, 0.4887]])\n",
            "torch.Size([5])\n",
            "tensor([0.1891, 0.3952, 0.9176, 0.8960, 0.4887])\n",
            "torch.Size([2, 2])\n",
            "torch.Size([2, 2])\n"
          ]
        }
      ],
      "source": [
        "a = torch.rand(1, 5)\n",
        "print(a.shape)\n",
        "print(a)\n",
        "\n",
        "b = a.squeeze(0)\n",
        "print(b.shape)\n",
        "print(b)\n",
        "\n",
        "c = torch.rand(2, 2)\n",
        "print(c.shape)\n",
        "\n",
        "d = c.squeeze(0)\n",
        "print(d.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gJa2LAdSgIg"
      },
      "source": [
        "You can see from the shapes that our 2-dimensional tensor is now\n",
        "1-dimensional, and if you look closely at the output of the cell above\n",
        "you'll see that printing `a` shows an \"extra\" set of square brackets\n",
        "`[]` due to having an extra dimension.\n",
        "\n",
        "You may only `squeeze()` dimensions of extent 1. See above where we try\n",
        "to squeeze a dimension of size 2 in `c`, and get back the same shape we\n",
        "started with. Calls to `squeeze()` and `unsqueeze()` can only act on\n",
        "dimensions of extent 1 because to do otherwise would change the number\n",
        "of elements in the tensor.\n",
        "\n",
        "Another place you might use `unsqueeze()` is to ease broadcasting.\n",
        "Recall the example above where we had the following code:\n",
        "\n",
        "``` {.python}\n",
        "a = torch.ones(4, 3, 2)\n",
        "\n",
        "c = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\n",
        "print(c)\n",
        "```\n",
        "\n",
        "The net effect of that was to broadcast the operation over dimensions 0\n",
        "and 2, causing the random, 3 x 1 tensor to be multiplied element-wise by\n",
        "every 3-element column in `a`.\n",
        "\n",
        "What if the random vector had just been 3-element vector? We'd lose the\n",
        "ability to do the broadcast, because the final dimensions would not\n",
        "match up according to the broadcasting rules. `unsqueeze()` comes to the\n",
        "rescue:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXJa0x2BSgIg",
        "outputId": "a94cec36-0dea-4e7e-9bea-7e6f2aab0140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1])\n",
            "tensor([[[0.6138, 0.6138],\n",
            "         [0.6854, 0.6854],\n",
            "         [0.0438, 0.0438]],\n",
            "\n",
            "        [[0.6138, 0.6138],\n",
            "         [0.6854, 0.6854],\n",
            "         [0.0438, 0.0438]],\n",
            "\n",
            "        [[0.6138, 0.6138],\n",
            "         [0.6854, 0.6854],\n",
            "         [0.0438, 0.0438]],\n",
            "\n",
            "        [[0.6138, 0.6138],\n",
            "         [0.6854, 0.6854],\n",
            "         [0.0438, 0.0438]]])\n"
          ]
        }
      ],
      "source": [
        "a = torch.ones(4, 3, 2)\n",
        "b = torch.rand(   3)     # trying to multiply a * b will give a runtime error\n",
        "c = b.unsqueeze(1)       # change to a 2-dimensional tensor, adding new dim at the end\n",
        "print(c.shape)\n",
        "print(a * c)             # broadcasting works again!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvfJy3a7SgIg"
      },
      "source": [
        "The `squeeze()` and `unsqueeze()` methods also have in-place versions,\n",
        "`squeeze_()` and `unsqueeze_()`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ4yLNyaSgIg",
        "outputId": "f5cd8abe-1818-43fb-867b-5aa3ebed1fdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 226, 226])\n",
            "torch.Size([1, 3, 226, 226])\n"
          ]
        }
      ],
      "source": [
        "batch_me = torch.rand(3, 226, 226)\n",
        "print(batch_me.shape)\n",
        "batch_me.unsqueeze_(0)\n",
        "print(batch_me.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShBX3pjQSgIg"
      },
      "source": [
        "Sometimes you'll want to change the shape of a tensor more radically,\n",
        "while still preserving the number of elements and their contents. One\n",
        "case where this happens is at the interface between a convolutional\n",
        "layer of a model and a linear layer of the model - this is common in\n",
        "image classification models. A convolution kernel will yield an output\n",
        "tensor of shape *features x width x height,* but the following linear\n",
        "layer expects a 1-dimensional input. `reshape()` will do this for you,\n",
        "provided that the dimensions you request yield the same number of\n",
        "elements as the input tensor has:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cWzx11mSgIg",
        "outputId": "a4b73252-4318-4750-cf27-f3c7ed0dd47d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 20, 20])\n",
            "torch.Size([2400])\n",
            "torch.Size([2400])\n"
          ]
        }
      ],
      "source": [
        "output3d = torch.rand(6, 20, 20)\n",
        "print(output3d.shape)\n",
        "\n",
        "input1d = output3d.reshape(6 * 20 * 20)\n",
        "print(input1d.shape)\n",
        "\n",
        "# can also call it as a method on the torch module:\n",
        "print(torch.reshape(output3d, (6 * 20 * 20,)).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dZoY9OYSgIg"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>The <code>(6 * 20 * 20,)</code> argument in the final line of the cellabove is because PyTorch expects a  when specifying atensor shape - but when the shape is the first argument of a method, itlets us cheat and just use a series of integers. Here, we had to add theparentheses and comma to convince the method that this is really aone-element tuple.</p>\n",
        "\n",
        "</div>\n",
        "\n",
        "When it can, `reshape()` will return a *view* on the tensor to be\n",
        "changed - that is, a separate tensor object looking at the same\n",
        "underlying region of memory. *This is important:* That means any change\n",
        "made to the source tensor will be reflected in the view on that tensor,\n",
        "unless you `clone()` it.\n",
        "\n",
        "There *are* conditions, beyond the scope of this introduction, where\n",
        "`reshape()` has to return a tensor carrying a copy of the data. For more\n",
        "information, see the\n",
        "[docs](https://pytorch.org/docs/stable/torch.html#torch.reshape).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoQ6wtfQSgIg"
      },
      "source": [
        "NumPy Bridge\n",
        "============\n",
        "\n",
        "In the section above on broadcasting, it was mentioned that PyTorch's\n",
        "broadcast semantics are compatible with NumPy's - but the kinship\n",
        "between PyTorch and NumPy goes even deeper than that.\n",
        "\n",
        "If you have existing ML or scientific code with data stored in NumPy\n",
        "ndarrays, you may wish to express that same data as PyTorch tensors,\n",
        "whether to take advantage of PyTorch's GPU acceleration, or its\n",
        "efficient abstractions for building ML models. It's easy to switch\n",
        "between ndarrays and PyTorch tensors:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQzXckmHSgIg",
        "outputId": "e127deb6-f406-4495-b745-6d092bb37069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "numpy_array = np.ones((2, 3))\n",
        "print(numpy_array)\n",
        "\n",
        "pytorch_tensor = torch.from_numpy(numpy_array)\n",
        "print(pytorch_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuc0-YA_SgIg"
      },
      "source": [
        "PyTorch creates a tensor of the same shape and containing the same data\n",
        "as the NumPy array, going so far as to keep NumPy's default 64-bit float\n",
        "data type.\n",
        "\n",
        "The conversion can just as easily go the other way:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ikgb2GsfSgIh",
        "outputId": "e2fa7dab-0928-439f-8b91-3621ae0036a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3838, 0.0171],\n",
            "        [0.2775, 0.0935],\n",
            "        [0.1874, 0.3136]])\n",
            "[[0.38376427 0.01713592]\n",
            " [0.27750254 0.09351915]\n",
            " [0.18740034 0.31359702]]\n"
          ]
        }
      ],
      "source": [
        "pytorch_rand = torch.rand(3, 2)\n",
        "print(pytorch_rand)\n",
        "\n",
        "numpy_rand = pytorch_rand.numpy()\n",
        "print(numpy_rand)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlV5PZp6SgIh"
      },
      "source": [
        "It is important to know that these converted objects are using *the same\n",
        "underlying memory* as their source objects, meaning that changes to one\n",
        "are reflected in the other:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gtj90lukSgIh",
        "outputId": "1d3dabca-b9a6-47f2-f6d8-44a6a9ca4554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.,  1.,  1.],\n",
            "        [ 1., 27.,  1.]], dtype=torch.float64)\n",
            "[[ 0.38376427  0.01713592]\n",
            " [ 0.27750254 13.        ]\n",
            " [ 0.18740034  0.31359702]]\n"
          ]
        }
      ],
      "source": [
        "numpy_array[1, 1] = 27\n",
        "print(pytorch_tensor)\n",
        "\n",
        "pytorch_rand[1, 1] = 13\n",
        "print(numpy_rand)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YVFIa_HZf7o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "r4fuZbtcOsre"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyBBwy7gOsrf"
      },
      "source": [
        "[Introduction](introyt1_tutorial.html) \\|\\|\n",
        "[Tensors](tensors_deeper_tutorial.html) \\|\\| **Autograd** \\|\\| [Building\n",
        "Models](modelsyt_tutorial.html) \\|\\| [TensorBoard\n",
        "Support](tensorboardyt_tutorial.html) \\|\\| [Training\n",
        "Models](trainingyt.html) \\|\\| [Model Understanding](captumyt.html)\n",
        "\n",
        "The Fundamentals of Autograd\n",
        "============================\n",
        "\n",
        "Follow along with the video below or on\n",
        "[youtube](https://www.youtube.com/watch?v=M0fX15_-xrY).\n",
        "\n",
        "``` {.python .jupyter-code-cell}\n",
        "from IPython.display import display, HTML\n",
        "html_code = \"\"\"\n",
        "<div style=\"margin-top:10px; margin-bottom:10px;\">\n",
        "  <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/M0fX15_-xrY\" frameborder=\"0\" allow=\"accelerometer; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
        "</div>\n",
        "\"\"\"\n",
        "display(HTML(html_code))\n",
        "```\n",
        "\n",
        "PyTorch's *Autograd* feature is part of what make PyTorch flexible and\n",
        "fast for building machine learning projects. It allows for the rapid and\n",
        "easy computation of multiple partial derivatives (also referred to as\n",
        "*gradients)* over a complex computation. This operation is central to\n",
        "backpropagation-based neural network learning.\n",
        "\n",
        "The power of autograd comes from the fact that it traces your\n",
        "computation dynamically *at runtime,* meaning that if your model has\n",
        "decision branches, or loops whose lengths are not known until runtime,\n",
        "the computation will still be traced correctly, and you'll get correct\n",
        "gradients to drive learning. This, combined with the fact that your\n",
        "models are built in Python, offers far more flexibility than frameworks\n",
        "that rely on static analysis of a more rigidly-structured model for\n",
        "computing gradients.\n",
        "\n",
        "What Do We Need Autograd For?\n",
        "-----------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSvfWgMMOsrg"
      },
      "source": [
        "A machine learning model is a *function*, with inputs and outputs. For\n",
        "this discussion, we'll treat the inputs as an *i*-dimensional vector\n",
        "$\\vec{x}$, with elements $x_{i}$. We can then express the model, *M*, as\n",
        "a vector-valued function of the input: $\\vec{y} =\n",
        "\\vec{M}(\\vec{x})$. (We treat the value of M's output as a vector because\n",
        "in general, a model may have any number of outputs.)\n",
        "\n",
        "Since we'll mostly be discussing autograd in the context of training,\n",
        "our output of interest will be the model's loss. The *loss function*\n",
        "L($\\vec{y}$) = L($\\vec{M}$($\\vec{x}$)) is a single-valued scalar\n",
        "function of the model's output. This function expresses how far off our\n",
        "model's prediction was from a particular input's *ideal* output. *Note:\n",
        "After this point, we will often omit the vector sign where it should be\n",
        "contextually clear - e.g.,* $y$ instead of $\\vec y$.\n",
        "\n",
        "In training a model, we want to minimize the loss. In the idealized case\n",
        "of a perfect model, that means adjusting its learning weights - that is,\n",
        "the adjustable parameters of the function - such that loss is zero for\n",
        "all inputs. In the real world, it means an iterative process of nudging\n",
        "the learning weights until we see that we get a tolerable loss for a\n",
        "wide variety of inputs.\n",
        "\n",
        "How do we decide how far and in which direction to nudge the weights? We\n",
        "want to *minimize* the loss, which means making its first derivative\n",
        "with respect to the input equal to 0:\n",
        "$\\frac{\\partial L}{\\partial x} = 0$.\n",
        "\n",
        "Recall, though, that the loss is not *directly* derived from the input,\n",
        "but a function of the model's output (which is a function of the input\n",
        "directly), $\\frac{\\partial L}{\\partial x}$ =\n",
        "$\\frac{\\partial {L({\\vec y})}}{\\partial x}$. By the chain rule of\n",
        "differential calculus, we have\n",
        "$\\frac{\\partial {L({\\vec y})}}{\\partial x}$ =\n",
        "$\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial x}$ =\n",
        "$\\frac{\\partial L}{\\partial y}\\frac{\\partial M(x)}{\\partial x}$.\n",
        "\n",
        "$\\frac{\\partial M(x)}{\\partial x}$ is where things get complex. The\n",
        "partial derivatives of the model's outputs with respect to its inputs,\n",
        "if we were to expand the expression using the chain rule again, would\n",
        "involve many local partial derivatives over every multiplied learning\n",
        "weight, every activation function, and every other mathematical\n",
        "transformation in the model. The full expression for each such partial\n",
        "derivative is the sum of the products of the local gradient of *every\n",
        "possible path* through the computation graph that ends with the variable\n",
        "whose gradient we are trying to measure.\n",
        "\n",
        "In particular, the gradients over the learning weights are of interest\n",
        "to us - they tell us *what direction to change each weight* to get the\n",
        "loss function closer to zero.\n",
        "\n",
        "Since the number of such local derivatives (each corresponding to a\n",
        "separate path through the model's computation graph) will tend to go up\n",
        "exponentially with the depth of a neural network, so does the complexity\n",
        "in computing them. This is where autograd comes in: It tracks the\n",
        "history of every computation. Every computed tensor in your PyTorch\n",
        "model carries a history of its input tensors and the function used to\n",
        "create it. Combined with the fact that PyTorch functions meant to act on\n",
        "tensors each have a built-in implementation for computing their own\n",
        "derivatives, this greatly speeds the computation of the local\n",
        "derivatives needed for learning.\n",
        "\n",
        "A Simple Example\n",
        "================\n",
        "\n",
        "That was a lot of theory - but what does it look like to use autograd in\n",
        "practice?\n",
        "\n",
        "Let's start with a straightforward example. First, we'll do some imports\n",
        "to let us graph our results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "O7CU-F7UOsrg"
      },
      "outputs": [],
      "source": [
        "# %matplotlib inline\n",
        "\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K4X6Dp7Osrg"
      },
      "source": [
        "Next, we'll create an input tensor full of evenly spaced values on the\n",
        "interval $[0, 2{\\pi}]$, and specify `requires_grad=True`. (Like most\n",
        "functions that create tensors, `torch.linspace()` accepts an optional\n",
        "`requires_grad` option.) Setting this flag means that in every\n",
        "computation that follows, autograd will be accumulating the history of\n",
        "the computation in the output tensors of that computation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9w1E4SiOsrh",
        "outputId": "cd4911de-b895-4056-a399-9150404b4701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0000, 0.2618, 0.5236, 0.7854, 1.0472, 1.3090, 1.5708, 1.8326, 2.0944,\n",
            "        2.3562, 2.6180, 2.8798, 3.1416, 3.4034, 3.6652, 3.9270, 4.1888, 4.4506,\n",
            "        4.7124, 4.9742, 5.2360, 5.4978, 5.7596, 6.0214, 6.2832],\n",
            "       requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL6WnFxGOsrh"
      },
      "source": [
        "Next, we'll perform a computation, and plot its output in terms of its\n",
        "inputs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "Kwd3cl1QOsrh",
        "outputId": "317de65e-adf3-411f-eee8-cba159fb450a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f08903d8390>]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "b = torch.sin(a)\n",
        "plt.plot(a.detach(), b.detach())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmFVgfggOsrh"
      },
      "source": [
        "Let's have a closer look at the tensor `b`. When we print it, we see an\n",
        "indicator that it is tracking its computation history:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbdM2_gSOsrh",
        "outputId": "f6779293-6a26-4ac0-d8df-2ec441a70ac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.0000e+00,  2.5882e-01,  5.0000e-01,  7.0711e-01,  8.6603e-01,\n",
            "         9.6593e-01,  1.0000e+00,  9.6593e-01,  8.6603e-01,  7.0711e-01,\n",
            "         5.0000e-01,  2.5882e-01, -8.7423e-08, -2.5882e-01, -5.0000e-01,\n",
            "        -7.0711e-01, -8.6603e-01, -9.6593e-01, -1.0000e+00, -9.6593e-01,\n",
            "        -8.6603e-01, -7.0711e-01, -5.0000e-01, -2.5882e-01,  1.7485e-07],\n",
            "       grad_fn=<SinBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am36FWzkOsrh"
      },
      "source": [
        "This `grad_fn` gives us a hint that when we execute the backpropagation\n",
        "step and compute gradients, we'll need to compute the derivative of\n",
        "$\\sin(x)$ for all this tensor's inputs.\n",
        "\n",
        "Let's perform some more computations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9w9_vV3Osrh",
        "outputId": "9bae4778-94dd-4a01-f806-dbd8dd8dee4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.0000e+00,  5.1764e-01,  1.0000e+00,  1.4142e+00,  1.7321e+00,\n",
            "         1.9319e+00,  2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,\n",
            "         1.0000e+00,  5.1764e-01, -1.7485e-07, -5.1764e-01, -1.0000e+00,\n",
            "        -1.4142e+00, -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00,\n",
            "        -1.7321e+00, -1.4142e+00, -1.0000e+00, -5.1764e-01,  3.4969e-07],\n",
            "       grad_fn=<MulBackward0>)\n",
            "tensor([ 1.0000e+00,  1.5176e+00,  2.0000e+00,  2.4142e+00,  2.7321e+00,\n",
            "         2.9319e+00,  3.0000e+00,  2.9319e+00,  2.7321e+00,  2.4142e+00,\n",
            "         2.0000e+00,  1.5176e+00,  1.0000e+00,  4.8236e-01, -3.5763e-07,\n",
            "        -4.1421e-01, -7.3205e-01, -9.3185e-01, -1.0000e+00, -9.3185e-01,\n",
            "        -7.3205e-01, -4.1421e-01,  4.7684e-07,  4.8236e-01,  1.0000e+00],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "c = 2 * b\n",
        "print(c)\n",
        "\n",
        "d = c + 1\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc3ciWzCOsrh"
      },
      "source": [
        "Finally, let's compute a single-element output. When you call\n",
        "`.backward()` on a tensor with no arguments, it expects the calling\n",
        "tensor to contain only a single element, as is the case when computing a\n",
        "loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeBdIl4eOsrh",
        "outputId": "e5b18e41-c881-4bd6-91d3-727184667267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(25., grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "out = d.sum()\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AWFkfclOsrh"
      },
      "source": [
        "Each `grad_fn` stored with our tensors allows you to walk the\n",
        "computation all the way back to its inputs with its `next_functions`\n",
        "property. We can see below that drilling down on this property on `d`\n",
        "shows us the gradient functions for all the prior tensors. Note that\n",
        "`a.grad_fn` is reported as `None`, indicating that this was an input to\n",
        "the function with no history of its own.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtDRmgJjOsri",
        "outputId": "1ea56d2f-9020-44f6-8050-b668b5bdaf67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d:\n",
            "<AddBackward0 object at 0x7f0899b630d0>\n",
            "((<MulBackward0 object at 0x7f0899c22b30>, 0), (None, 0))\n",
            "((<SinBackward0 object at 0x7f0899b630d0>, 0), (None, 0))\n",
            "((<AccumulateGrad object at 0x7f08905d4850>, 0),)\n",
            "()\n",
            "\n",
            "c:\n",
            "<MulBackward0 object at 0x7f08905d49a0>\n",
            "\n",
            "b:\n",
            "<SinBackward0 object at 0x7f08905d49a0>\n",
            "\n",
            "a:\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print('d:')\n",
        "print(d.grad_fn)\n",
        "print(d.grad_fn.next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
        "print('\\nc:')\n",
        "print(c.grad_fn)\n",
        "print('\\nb:')\n",
        "print(b.grad_fn)\n",
        "print('\\na:')\n",
        "print(a.grad_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGc4Qop0Osri"
      },
      "source": [
        "With all this machinery in place, how do we get derivatives out? You\n",
        "call the `backward()` method on the output, and check the input's `grad`\n",
        "property to inspect the gradients:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "4LgaGsV7Osri",
        "outputId": "470ae108-232a-40ea-eaa4-5af9744c4f5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,  1.0000e+00,\n",
            "         5.1764e-01, -8.7423e-08, -5.1764e-01, -1.0000e+00, -1.4142e+00,\n",
            "        -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00, -1.7321e+00,\n",
            "        -1.4142e+00, -1.0000e+00, -5.1764e-01,  2.3850e-08,  5.1764e-01,\n",
            "         1.0000e+00,  1.4142e+00,  1.7321e+00,  1.9319e+00,  2.0000e+00])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f089055dcd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "out.backward()\n",
        "print(a.grad)\n",
        "plt.plot(a.detach(), a.grad.detach())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L15ocy-nOsri"
      },
      "source": [
        "Recall the computation steps we took to get here:\n",
        "\n",
        "``` {.python}\n",
        "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
        "b = torch.sin(a)\n",
        "c = 2 * b\n",
        "d = c + 1\n",
        "out = d.sum()\n",
        "```\n",
        "\n",
        "Adding a constant, as we did to compute `d`, does not change the\n",
        "derivative. That leaves $c = 2 * b = 2 * \\sin(a)$, the derivative of\n",
        "which should be $2 * \\cos(a)$. Looking at the graph above, that's just\n",
        "what we see.\n",
        "\n",
        "Be aware that only *leaf nodes* of the computation have their gradients\n",
        "computed. If you tried, for example, `print(c.grad)` you'd get back\n",
        "`None`. In this simple example, only the input is a leaf node, so only\n",
        "it has gradients computed.\n",
        "\n",
        "Autograd in Training\n",
        "====================\n",
        "\n",
        "We've had a brief look at how autograd works, but how does it look when\n",
        "it's used for its intended purpose? Let's define a small model and\n",
        "examine how it changes after a single training batch. First, define a\n",
        "few constants, our model, and some stand-ins for inputs and outputs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "L6KkXBC8Osri"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "DIM_IN = 1000\n",
        "HIDDEN_SIZE = 100\n",
        "DIM_OUT = 10\n",
        "\n",
        "class TinyModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TinyModel, self).__init__()\n",
        "\n",
        "        self.layer1 = torch.nn.Linear(DIM_IN, HIDDEN_SIZE)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.layer2 = torch.nn.Linear(HIDDEN_SIZE, DIM_OUT)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "some_input = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=False)\n",
        "ideal_output = torch.randn(BATCH_SIZE, DIM_OUT, requires_grad=False)\n",
        "\n",
        "model = TinyModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx8HDVEOOsri"
      },
      "source": [
        "One thing you might notice is that we never specify `requires_grad=True`\n",
        "for the model's layers. Within a subclass of `torch.nn.Module`, it's\n",
        "assumed that we want to track gradients on the layers' weights for\n",
        "learning.\n",
        "\n",
        "If we look at the layers of the model, we can examine the values of the\n",
        "weights, and verify that no gradients have been computed yet:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wjtT4m-Osri",
        "outputId": "6a26b918-7940-4bd9-893f-0f226e7c9f54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0578,  0.0164, -0.0324,  0.0208,  0.0642, -0.0139,  0.0779,  0.0258,\n",
            "        -0.0904, -0.0791], grad_fn=<SliceBackward0>)\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(model.layer2.weight[0][0:10]) # just a small slice\n",
        "print(model.layer2.weight.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfnEM5y5Osri"
      },
      "source": [
        "Let's see how this changes when we run through one training batch. For a\n",
        "loss function, we'll just use the square of the Euclidean distance\n",
        "between our `prediction` and the `ideal_output`, and we'll use a basic\n",
        "stochastic gradient descent optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSWWMqDROsri",
        "outputId": "c7cf2d6d-6759-4e9b-b911-212cccb95e39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(196.7258, grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005)\n",
        "\n",
        "prediction = model(some_input)\n",
        "\n",
        "loss = (ideal_output - prediction).pow(2).sum()\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q9HxvlSOsri"
      },
      "source": [
        "Now, let's call `loss.backward()` and see what happens:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwJNi9JgOsri",
        "outputId": "2ce534c5-0529-44b3-b699-f07ae9b3a8df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0578,  0.0164, -0.0324,  0.0208,  0.0642, -0.0139,  0.0779,  0.0258,\n",
            "        -0.0904, -0.0791], grad_fn=<SliceBackward0>)\n",
            "tensor([6.3177, 8.6403, 6.8705, 0.2129, 1.1862, 4.9568, 3.0792, 7.8493, 1.8272,\n",
            "        8.8125])\n"
          ]
        }
      ],
      "source": [
        "loss.backward()\n",
        "print(model.layer2.weight[0][0:10])\n",
        "print(model.layer2.weight.grad[0][0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK9zyU6BOsri"
      },
      "source": [
        "We can see that the gradients have been computed for each learning\n",
        "weight, but the weights remain unchanged, because we haven't run the\n",
        "optimizer yet. The optimizer is responsible for updating model weights\n",
        "based on the computed gradients.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-_ZF4yGOsri",
        "outputId": "ac8af052-0ba2-48f0-fbb7-3f47e3a4cc1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0894, -0.0268, -0.0667,  0.0197,  0.0583, -0.0387,  0.0625, -0.0134,\n",
            "        -0.0995, -0.1232], grad_fn=<SliceBackward0>)\n",
            "tensor([6.3177, 8.6403, 6.8705, 0.2129, 1.1862, 4.9568, 3.0792, 7.8493, 1.8272,\n",
            "        8.8125])\n"
          ]
        }
      ],
      "source": [
        "optimizer.step()\n",
        "print(model.layer2.weight[0][0:10])\n",
        "print(model.layer2.weight.grad[0][0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd2RGtGXOsri"
      },
      "source": [
        "You should see that `layer2`'s weights have changed.\n",
        "\n",
        "One important thing about the process: After calling `optimizer.step()`,\n",
        "you need to call `optimizer.zero_grad()`, or else every time you run\n",
        "`loss.backward()`, the gradients on the learning weights will\n",
        "accumulate:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJnY4LWLOsrj"
      },
      "outputs": [],
      "source": [
        "print(model.layer2.weight.grad[0][0:10])\n",
        "\n",
        "for i in range(0, 5):\n",
        "    prediction = model(some_input)\n",
        "    loss = (ideal_output - prediction).pow(2).sum()\n",
        "    loss.backward()\n",
        "\n",
        "print(model.layer2.weight.grad[0][0:10])\n",
        "\n",
        "optimizer.zero_grad(set_to_none=False)\n",
        "\n",
        "print(model.layer2.weight.grad[0][0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWsVdaAvOsrj"
      },
      "source": [
        "After running the cell above, you should see that after running\n",
        "`loss.backward()` multiple times, the magnitudes of most of the\n",
        "gradients will be much larger. Failing to zero the gradients before\n",
        "running your next training batch will cause the gradients to blow up in\n",
        "this manner, causing incorrect and unpredictable learning results.\n",
        "\n",
        "Turning Autograd Off and On\n",
        "===========================\n",
        "\n",
        "There are situations where you will need fine-grained control over\n",
        "whether autograd is enabled. There are multiple ways to do this,\n",
        "depending on the situation.\n",
        "\n",
        "The simplest is to change the `requires_grad` flag on a tensor directly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHdisLQJOsrj",
        "outputId": "0b48d1d3-a9d4-40b8-c849-428fad941a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]], requires_grad=True)\n",
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.]], grad_fn=<MulBackward0>)\n",
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.]])\n"
          ]
        }
      ],
      "source": [
        "a = torch.ones(2, 3, requires_grad=True)\n",
        "print(a)\n",
        "\n",
        "b1 = 2 * a\n",
        "print(b1)\n",
        "\n",
        "a.requires_grad = False\n",
        "b2 = 2 * a\n",
        "print(b2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4n1jsR7Osrj"
      },
      "source": [
        "In the cell above, we see that `b1` has a `grad_fn` (i.e., a traced\n",
        "computation history), which is what we expect, since it was derived from\n",
        "a tensor, `a`, that had autograd turned on. When we turn off autograd\n",
        "explicitly with `a.requires_grad = False`, computation history is no\n",
        "longer tracked, as we see when we compute `b2`.\n",
        "\n",
        "If you only need autograd turned off temporarily, a better way is to use\n",
        "the `torch.no_grad()`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn8E5akzOsrj",
        "outputId": "9790cacc-5c50-47d9-d074-97053340c3a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
            "tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]])\n",
            "tensor([[6., 6., 6.],\n",
            "        [6., 6., 6.]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "a = torch.ones(2, 3, requires_grad=True) * 2\n",
        "b = torch.ones(2, 3, requires_grad=True) * 3\n",
        "\n",
        "c1 = a + b\n",
        "print(c1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    c2 = a + b\n",
        "\n",
        "print(c2)\n",
        "\n",
        "c3 = a * b\n",
        "print(c3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MS6LyXbOsrj"
      },
      "source": [
        "`torch.no_grad()` can also be used as a function or method decorator:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FHhJhTlOsrj",
        "outputId": "20dcd661-ab51-4ced-a317-d3625674e121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
            "tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]])\n"
          ]
        }
      ],
      "source": [
        "def add_tensors1(x, y):\n",
        "    return x + y\n",
        "\n",
        "@torch.no_grad()\n",
        "def add_tensors2(x, y):\n",
        "    return x + y\n",
        "\n",
        "\n",
        "a = torch.ones(2, 3, requires_grad=True) * 2\n",
        "b = torch.ones(2, 3, requires_grad=True) * 3\n",
        "\n",
        "c1 = add_tensors1(a, b)\n",
        "print(c1)\n",
        "\n",
        "c2 = add_tensors2(a, b)\n",
        "print(c2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VmyXu-_Osrj"
      },
      "source": [
        "There's a corresponding context manager, `torch.enable_grad()`, for\n",
        "turning autograd on when it isn't already. It may also be used as a\n",
        "decorator.\n",
        "\n",
        "Finally, you may have a tensor that requires gradient tracking, but you\n",
        "want a copy that does not. For this we have the `Tensor` object's\n",
        "`detach()` method - it creates a copy of the tensor that is *detached*\n",
        "from the computation history:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS9CviqPOsrj",
        "outputId": "def89a12-4cac-4f19-b786-56e428463c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1783, 0.2207, 0.4478, 0.2368, 0.8141], requires_grad=True)\n",
            "tensor([0.1783, 0.2207, 0.4478, 0.2368, 0.8141])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(5, requires_grad=True)\n",
        "y = x.detach()\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z7DMSgqOsrj"
      },
      "source": [
        "We did this above when we wanted to graph some of our tensors. This is\n",
        "because `matplotlib` expects a NumPy array as input, and the implicit\n",
        "conversion from a PyTorch tensor to a NumPy array is not enabled for\n",
        "tensors with requires\\_grad=True. Making a detached copy lets us move\n",
        "forward.\n",
        "\n",
        "Autograd and In-place Operations\n",
        "================================\n",
        "\n",
        "In every example in this notebook so far, we've used variables to\n",
        "capture the intermediate values of a computation. Autograd needs these\n",
        "intermediate values to perform gradient computations. *For this reason,\n",
        "you must be careful about using in-place operations when using\n",
        "autograd.* Doing so can destroy information you need to compute\n",
        "derivatives in the `backward()` call. PyTorch will even stop you if you\n",
        "attempt an in-place operation on leaf variable that requires autograd,\n",
        "as shown below.\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>The following code cell throws a runtime error. This is expected.<pre><code>a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
        "torch.sin_(a)</code></pre></p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkT_TZVEOsro"
      },
      "source": [
        "Autograd Profiler\n",
        "=================\n",
        "\n",
        "Autograd tracks every step of your computation in detail. Such a\n",
        "computation history, combined with timing information, would make a\n",
        "handy profiler - and autograd has that feature baked in. Here's a quick\n",
        "example usage:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8K2HlqmVOsro",
        "outputId": "7971445d-794b-468e-94ac-e42131913742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-76-cb25284932df>:11: FutureWarning: The attribute `use_cuda` will be deprecated soon, please use ``use_device = 'cuda'`` instead.\n",
            "  with torch.autograd.profiler.profile(use_cuda=run_on_gpu) as prf:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "          cudaEventRecord        39.60%       5.848ms        39.60%       5.848ms       1.462us       0.000us         0.00%       0.000us       0.000us          4000  \n",
            "                aten::mul        30.62%       4.521ms        30.62%       4.521ms       4.521us       7.845ms        50.93%       7.845ms       7.845us          1000  \n",
            "                aten::div        29.69%       4.385ms        29.69%       4.385ms       4.385us       7.558ms        49.07%       7.558ms       7.558us          1000  \n",
            "    cudaDeviceSynchronize         0.09%      12.784us         0.09%      12.784us      12.784us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 14.767ms\n",
            "Self CUDA time total: 15.403ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cpu')\n",
        "run_on_gpu = False\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    run_on_gpu = True\n",
        "\n",
        "x = torch.randn(2, 3, requires_grad=True)\n",
        "y = torch.rand(2, 3, requires_grad=True)\n",
        "z = torch.ones(2, 3, requires_grad=True)\n",
        "\n",
        "with torch.autograd.profiler.profile(use_cuda=run_on_gpu) as prf:\n",
        "    for _ in range(1000):\n",
        "        z = (z / x) * y\n",
        "\n",
        "print(prf.key_averages().table(sort_by='self_cpu_time_total'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXcNax5WOsro"
      },
      "source": [
        "The profiler can also label individual sub-blocks of code, break out the\n",
        "data by input tensor shape, and export data as a Chrome tracing tools\n",
        "file. For full details of the API, see the\n",
        "[documentation](https://pytorch.org/docs/stable/autograd.html#profiler).\n",
        "\n",
        "Advanced Topic: More Autograd Detail and the High-Level API\n",
        "===========================================================\n",
        "\n",
        "If you have a function with an n-dimensional input and m-dimensional\n",
        "output, $\\vec{y}=f(\\vec{x})$, the complete gradient is a matrix of the\n",
        "derivative of every output with respect to every input, called the\n",
        "*Jacobian:*\n",
        "\n",
        "$$\\begin{aligned}\n",
        "J\n",
        "=\n",
        "\\left(\\begin{array}{ccc}\n",
        "\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
        "\\vdots & \\ddots & \\vdots\\\\\n",
        "\\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "\\end{array}\\right)\n",
        "\\end{aligned}$$\n",
        "\n",
        "If you have a second function, $l=g\\left(\\vec{y}\\right)$ that takes\n",
        "m-dimensional input (that is, the same dimensionality as the output\n",
        "above), and returns a scalar output, you can express its gradients with\n",
        "respect to $\\vec{y}$ as a column vector,\n",
        "$v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$\n",
        "- which is really just a one-column Jacobian.\n",
        "\n",
        "More concretely, imagine the first function as your PyTorch model (with\n",
        "potentially many inputs and many outputs) and the second function as a\n",
        "loss function (with the model's output as input, and the loss value as\n",
        "the scalar output).\n",
        "\n",
        "If we multiply the first function's Jacobian by the gradient of the\n",
        "second function, and apply the chain rule, we get:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
        "\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
        "\\vdots & \\ddots & \\vdots\\\\\n",
        "\\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "\\end{array}\\right)\\left(\\begin{array}{c}\n",
        "\\frac{\\partial l}{\\partial y_{1}}\\\\\n",
        "\\vdots\\\\\n",
        "\\frac{\\partial l}{\\partial y_{m}}\n",
        "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
        "\\frac{\\partial l}{\\partial x_{1}}\\\\\n",
        "\\vdots\\\\\n",
        "\\frac{\\partial l}{\\partial x_{n}}\n",
        "\\end{array}\\right)\n",
        "\\end{aligned}$$\n",
        "\n",
        "Note: You could also use the equivalent operation $v^{T}\\cdot J$, and\n",
        "get back a row vector.\n",
        "\n",
        "The resulting column vector is the *gradient of the second function with\n",
        "respect to the inputs of the first* - or in the case of our model and\n",
        "loss function, the gradient of the loss with respect to the model\n",
        "inputs.\n",
        "\n",
        "**\\`\\`torch.autograd\\`\\` is an engine for computing these products.**\n",
        "This is how we accumulate the gradients over the learning weights during\n",
        "the backward pass.\n",
        "\n",
        "For this reason, the `backward()` call can *also* take an optional\n",
        "vector input. This vector represents a set of gradients over the tensor,\n",
        "which are multiplied by the Jacobian of the autograd-traced tensor that\n",
        "precedes it. Let's try a specific example with a small vector:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGcCwh0IOsro",
        "outputId": "62a06f24-fcca-4853-9da7-e887e3160249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1465.6471,  361.6230,  506.8873], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "y = x * 2\n",
        "while y.data.norm() < 1000:\n",
        "    y = y * 2\n",
        "\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCjSTDDMOsro"
      },
      "source": [
        "If we tried to call `y.backward()` now, we'd get a runtime error and a\n",
        "message that gradients can only be *implicitly* computed for scalar\n",
        "outputs. For a multi-dimensional output, autograd expects us to provide\n",
        "gradients for those three outputs that it can multiply into the\n",
        "Jacobian:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3U9jXdcOsro",
        "outputId": "9c4d2608-0673-4a07-b097-b7f47d0669eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
          ]
        }
      ],
      "source": [
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float) # stand-in for gradients\n",
        "y.backward(v)\n",
        "\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAyw_1B3Osro"
      },
      "source": [
        "(Note that the output gradients are all related to powers of two - which\n",
        "we'd expect from a repeated doubling operation.)\n",
        "\n",
        "The High-Level API\n",
        "==================\n",
        "\n",
        "There is an API on autograd that gives you direct access to important\n",
        "differential matrix and vector operations. In particular, it allows you\n",
        "to calculate the Jacobian and the *Hessian* matrices of a particular\n",
        "function for particular inputs. (The Hessian is like the Jacobian, but\n",
        "expresses all partial *second* derivatives.) It also provides methods\n",
        "for taking vector products with these matrices.\n",
        "\n",
        "Let's take the Jacobian of a simple function, evaluated for a 2\n",
        "single-element inputs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHFE606ZOsro",
        "outputId": "cab05c6c-193f-4b8c-c003-0710097cacf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([0.3736]), tensor([0.9641]))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[2.9060]]), tensor([[3.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "def exp_adder(x, y):\n",
        "    return 2 * x.exp() + 3 * y\n",
        "\n",
        "inputs = (torch.rand(1), torch.rand(1)) # arguments for the function\n",
        "print(inputs)\n",
        "torch.autograd.functional.jacobian(exp_adder, inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75nOaik5Osro"
      },
      "source": [
        "If you look closely, the first output should equal $2e^x$ (since the\n",
        "derivative of $e^x$ is $e^x$), and the second value should be 3.\n",
        "\n",
        "You can, of course, do this with higher-order tensors:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OXPn_dPOsro",
        "outputId": "97817b34-d33d-4cb8-bae8-5382d46ca08e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([0.3364, 0.4112, 0.4752]), tensor([0.4107, 0.8640, 0.6348]))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[2.7997, 0.0000, 0.0000],\n",
              "         [0.0000, 3.0174, 0.0000],\n",
              "         [0.0000, 0.0000, 3.2168]]),\n",
              " tensor([[3., 0., 0.],\n",
              "         [0., 3., 0.],\n",
              "         [0., 0., 3.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "inputs = (torch.rand(3), torch.rand(3)) # arguments for the function\n",
        "print(inputs)\n",
        "torch.autograd.functional.jacobian(exp_adder, inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0ijP6McOsro"
      },
      "source": [
        "The `torch.autograd.functional.hessian()` method works identically\n",
        "(assuming your function is twice differentiable), but returns a matrix\n",
        "of all second derivatives.\n",
        "\n",
        "There is also a function to directly compute the vector-Jacobian\n",
        "product, if you provide the vector:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye0rU8aEOsrp",
        "outputId": "b8883b2b-5dfe-4525-c344-020c77896be7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 509.9130, -536.9694,  703.9870]),\n",
              " tensor([1.0240e+02, 1.0240e+03, 1.0240e-01]))"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "def do_some_doubling(x):\n",
        "    y = x * 2\n",
        "    while y.data.norm() < 1000:\n",
        "        y = y * 2\n",
        "    return y\n",
        "\n",
        "inputs = torch.randn(3)\n",
        "my_gradients = torch.tensor([0.1, 1.0, 0.0001])\n",
        "torch.autograd.functional.vjp(do_some_doubling, inputs, v=my_gradients)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PdwkeAqOsrp"
      },
      "source": [
        "The `torch.autograd.functional.jvp()` method performs the same matrix\n",
        "multiplication as `vjp()` with the operands reversed. The `vhp()` and\n",
        "`hvp()` methods do the same for a vector-Hessian product.\n",
        "\n",
        "For more information, including performance notes on the [docs for the\n",
        "functional\n",
        "API](https://pytorch.org/docs/stable/autograd.html#functional-higher-level-api)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yerU0xRQsZD"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ9VuLL3QsZE"
      },
      "source": [
        "[Introduction](introyt1_tutorial.html) \\|\\|\n",
        "[Tensors](tensors_deeper_tutorial.html) \\|\\|\n",
        "[Autograd](autogradyt_tutorial.html) \\|\\| **Building Models** \\|\\|\n",
        "[TensorBoard Support](tensorboardyt_tutorial.html) \\|\\| [Training\n",
        "Models](trainingyt.html) \\|\\| [Model Understanding](captumyt.html)\n",
        "\n",
        "Building Models with PyTorch\n",
        "============================\n",
        "\n",
        "Follow along with the video below or on\n",
        "[youtube](https://www.youtube.com/watch?v=OSqIP-mOWOI).\n",
        "\n",
        "``` {.python .jupyter-code-cell}\n",
        "from IPython.display import display, HTML\n",
        "html_code = \"\"\"\n",
        "<div style=\"margin-top:10px; margin-bottom:10px;\">\n",
        "  <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/OSqIP-mOWOI\" frameborder=\"0\" allow=\"accelerometer; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
        "</div>\n",
        "\"\"\"\n",
        "display(HTML(html_code))\n",
        "```\n",
        "\n",
        "`torch.nn.Module` and `torch.nn.Parameter`\n",
        "------------------------------------------\n",
        "\n",
        "In this video, we'll be discussing some of the tools PyTorch makes\n",
        "available for building deep learning networks.\n",
        "\n",
        "Except for `Parameter`, the classes we discuss in this video are all\n",
        "subclasses of `torch.nn.Module`. This is the PyTorch base class meant to\n",
        "encapsulate behaviors specific to PyTorch Models and their components.\n",
        "\n",
        "One important behavior of `torch.nn.Module` is registering parameters.\n",
        "If a particular `Module` subclass has learning weights, these weights\n",
        "are expressed as instances of `torch.nn.Parameter`. The `Parameter`\n",
        "class is a subclass of `torch.Tensor`, with the special behavior that\n",
        "when they are assigned as attributes of a `Module`, they are added to\n",
        "the list of that modules parameters. These parameters may be accessed\n",
        "through the `parameters()` method on the `Module` class.\n",
        "\n",
        "As a simple example, here's a very simple model with two linear layers\n",
        "and an activation function. We'll create an instance of it and ask it to\n",
        "report on its parameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KIRUD7pQsZF",
        "outputId": "578542f8-0d71-4541-f2c6-94addcd41e8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model:\n",
            "TinyModel(\n",
            "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
            "  (activation): ReLU()\n",
            "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n",
            "\n",
            "\n",
            "Just one layer:\n",
            "Linear(in_features=200, out_features=10, bias=True)\n",
            "\n",
            "\n",
            "Model params:\n",
            "Parameter containing:\n",
            "tensor([[-0.0660,  0.0478, -0.0288,  ...,  0.0162,  0.0195,  0.0897],\n",
            "        [ 0.0243, -0.0374, -0.0433,  ...,  0.0417,  0.0148, -0.0725],\n",
            "        [-0.0959, -0.0544, -0.0092,  ..., -0.0745,  0.0534,  0.0166],\n",
            "        ...,\n",
            "        [ 0.0981,  0.0826, -0.0586,  ...,  0.0799,  0.0394, -0.0395],\n",
            "        [-0.0136, -0.0432, -0.0400,  ...,  0.0031,  0.0923, -0.0946],\n",
            "        [-0.0271, -0.0012, -0.0947,  ...,  0.0419,  0.0254,  0.0930]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0508,  0.0166, -0.0871, -0.0174,  0.0944,  0.0273,  0.0831, -0.0961,\n",
            "         0.0085, -0.0208, -0.0344, -0.0505,  0.0488, -0.0067, -0.0428,  0.0489,\n",
            "         0.0444,  0.0900,  0.0620,  0.0518,  0.0936,  0.0831, -0.0188,  0.0487,\n",
            "         0.0976, -0.0658, -0.0008,  0.0974,  0.0943,  0.0274,  0.0654, -0.0064,\n",
            "        -0.0286, -0.0274, -0.0914, -0.0295, -0.0918,  0.0126, -0.0914,  0.0324,\n",
            "        -0.0845, -0.0933, -0.0107, -0.0260, -0.0272, -0.0226, -0.0708, -0.0375,\n",
            "         0.0626, -0.0573,  0.0087,  0.0425, -0.0119,  0.0218, -0.0204,  0.0630,\n",
            "         0.0199,  0.0534, -0.0339,  0.0184, -0.0961, -0.0280, -0.0947, -0.0182,\n",
            "         0.0255,  0.0232, -0.0980, -0.0029,  0.0418,  0.0786, -0.0464, -0.0401,\n",
            "        -0.0466, -0.0679, -0.0880,  0.0613, -0.0541,  0.0822, -0.0488,  0.0615,\n",
            "        -0.0558,  0.0436, -0.0804, -0.0443, -0.0346,  0.0037,  0.0609,  0.0635,\n",
            "         0.0412,  0.0717,  0.0372,  0.0436,  0.0131, -0.0135, -0.0592,  0.0979,\n",
            "        -0.0607,  0.0519, -0.0516,  0.0992, -0.0806, -0.0536,  0.0081, -0.0989,\n",
            "         0.0762, -0.0175, -0.0036,  0.0903, -0.0188, -0.0362,  0.0362, -0.0368,\n",
            "         0.0481, -0.0482, -0.0296, -0.0913, -0.0233,  0.0788, -0.0173,  0.0784,\n",
            "         0.0419, -0.0348, -0.0566,  0.0089, -0.0638, -0.0538,  0.0106,  0.0749,\n",
            "         0.0833, -0.0628, -0.0671, -0.0667, -0.0660,  0.0471,  0.0524, -0.0149,\n",
            "         0.0091, -0.0272, -0.0179,  0.0645, -0.0826, -0.0657, -0.0975,  0.0900,\n",
            "        -0.0589,  0.0145, -0.0148,  0.0151, -0.0835,  0.0761,  0.0319,  0.0163,\n",
            "         0.0007,  0.0666,  0.0438,  0.0546,  0.0622, -0.0134, -0.0880,  0.0924,\n",
            "        -0.0757, -0.0539, -0.0582,  0.0412, -0.0884, -0.0379,  0.0885, -0.0890,\n",
            "         0.0370, -0.0286,  0.0728,  0.0263,  0.0394, -0.0275, -0.0340,  0.0825,\n",
            "         0.0046,  0.0753,  0.0476,  0.0273, -0.0112, -0.0375, -0.0626,  0.0332,\n",
            "         0.0810,  0.0687,  0.0430, -0.0312, -0.0992, -0.0293,  0.0835,  0.0514,\n",
            "         0.0659, -0.0692, -0.0083, -0.0772,  0.0640,  0.0448,  0.0126, -0.0147],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0095,  0.0491,  0.0389,  ..., -0.0261, -0.0254, -0.0296],\n",
            "        [ 0.0569,  0.0664,  0.0654,  ..., -0.0340, -0.0270,  0.0044],\n",
            "        [ 0.0157,  0.0054, -0.0057,  ..., -0.0324,  0.0201,  0.0667],\n",
            "        ...,\n",
            "        [ 0.0564,  0.0322, -0.0571,  ..., -0.0148, -0.0233,  0.0046],\n",
            "        [-0.0074, -0.0413, -0.0116,  ...,  0.0612, -0.0358,  0.0184],\n",
            "        [ 0.0405, -0.0528, -0.0028,  ..., -0.0491,  0.0630,  0.0174]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0026,  0.0279, -0.0503,  0.0022,  0.0102, -0.0437,  0.0277, -0.0479,\n",
            "         0.0144,  0.0184], requires_grad=True)\n",
            "\n",
            "\n",
            "Layer params:\n",
            "Parameter containing:\n",
            "tensor([[-0.0095,  0.0491,  0.0389,  ..., -0.0261, -0.0254, -0.0296],\n",
            "        [ 0.0569,  0.0664,  0.0654,  ..., -0.0340, -0.0270,  0.0044],\n",
            "        [ 0.0157,  0.0054, -0.0057,  ..., -0.0324,  0.0201,  0.0667],\n",
            "        ...,\n",
            "        [ 0.0564,  0.0322, -0.0571,  ..., -0.0148, -0.0233,  0.0046],\n",
            "        [-0.0074, -0.0413, -0.0116,  ...,  0.0612, -0.0358,  0.0184],\n",
            "        [ 0.0405, -0.0528, -0.0028,  ..., -0.0491,  0.0630,  0.0174]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0026,  0.0279, -0.0503,  0.0022,  0.0102, -0.0437,  0.0277, -0.0479,\n",
            "         0.0144,  0.0184], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "class TinyModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TinyModel, self).__init__()\n",
        "\n",
        "        self.linear1 = torch.nn.Linear(100, 200)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        self.linear2 = torch.nn.Linear(200, 10)\n",
        "        self.softmax = torch.nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "tinymodel = TinyModel()\n",
        "\n",
        "print('The model:')\n",
        "print(tinymodel)\n",
        "\n",
        "print('\\n\\nJust one layer:')\n",
        "print(tinymodel.linear2)\n",
        "\n",
        "print('\\n\\nModel params:')\n",
        "for param in tinymodel.parameters():\n",
        "    print(param)\n",
        "\n",
        "print('\\n\\nLayer params:')\n",
        "for param in tinymodel.linear2.parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSlM3jEwQsZG"
      },
      "source": [
        "This shows the fundamental structure of a PyTorch model: there is an\n",
        "`__init__()` method that defines the layers and other components of a\n",
        "model, and a `forward()` method where the computation gets done. Note\n",
        "that we can print the model, or any of its submodules, to learn about\n",
        "its structure.\n",
        "\n",
        "Common Layer Types\n",
        "==================\n",
        "\n",
        "Linear Layers\n",
        "-------------\n",
        "\n",
        "The most basic type of neural network layer is a *linear* or *fully\n",
        "connected* layer. This is a layer where every input influences every\n",
        "output of the layer to a degree specified by the layer's weights. If a\n",
        "model has *m* inputs and *n* outputs, the weights will be an *m* x *n*\n",
        "matrix. For example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VCAu9qvQsZG",
        "outputId": "f8dba1dd-2d59-4913-90f8-307fea275431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "tensor([[0.1372, 0.2218, 0.3781]])\n",
            "\n",
            "\n",
            "Weight and Bias parameters:\n",
            "Parameter containing:\n",
            "tensor([[-0.5340, -0.3371, -0.2644],\n",
            "        [ 0.4428,  0.1569, -0.0083]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.3139,  0.3567], requires_grad=True)\n",
            "\n",
            "\n",
            "Output:\n",
            "tensor([[-0.5619,  0.4491]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "lin = torch.nn.Linear(3, 2)\n",
        "x = torch.rand(1, 3)\n",
        "print('Input:')\n",
        "print(x)\n",
        "\n",
        "print('\\n\\nWeight and Bias parameters:')\n",
        "for param in lin.parameters():\n",
        "    print(param)\n",
        "\n",
        "y = lin(x)\n",
        "print('\\n\\nOutput:')\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfd_OtpuQsZG"
      },
      "source": [
        "If you do the matrix multiplication of `x` by the linear layer's\n",
        "weights, and add the biases, you'll find that you get the output vector\n",
        "`y`.\n",
        "\n",
        "One other important feature to note: When we checked the weights of our\n",
        "layer with `lin.weight`, it reported itself as a `Parameter` (which is a\n",
        "subclass of `Tensor`), and let us know that it's tracking gradients with\n",
        "autograd. This is a default behavior for `Parameter` that differs from\n",
        "`Tensor`.\n",
        "\n",
        "Linear layers are used widely in deep learning models. One of the most\n",
        "common places you'll see them is in classifier models, which will\n",
        "usually have one or more linear layers at the end, where the last layer\n",
        "will have *n* outputs, where *n* is the number of classes the classifier\n",
        "addresses.\n",
        "\n",
        "Convolutional Layers\n",
        "====================\n",
        "\n",
        "*Convolutional* layers are built to handle data with a high degree of\n",
        "spatial correlation. They are very commonly used in computer vision,\n",
        "where they detect close groupings of features which the compose into\n",
        "higher-level features. They pop up in other contexts too - for example,\n",
        "in NLP applications, where a word's immediate context (that is, the\n",
        "other words nearby in the sequence) can affect the meaning of a\n",
        "sentence.\n",
        "\n",
        "We saw convolutional layers in action in LeNet5 in an earlier video:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "nsoDM5KEQsZG"
      },
      "outputs": [],
      "source": [
        "import torch.functional as F\n",
        "\n",
        "\n",
        "class LeNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
        "        self.fc2 = torch.nn.Linear(120, 84)\n",
        "        self.fc3 = torch.nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPB_NJMJQsZG"
      },
      "source": [
        "Let's break down what's happening in the convolutional layers of this\n",
        "model. Starting with `conv1`:\n",
        "\n",
        "-   LeNet5 is meant to take in a 1x32x32 black & white image. **The\n",
        "    first argument to a convolutional layer's constructor is the number\n",
        "    of input channels.** Here, it is 1. If we were building this model\n",
        "    to look at 3-color channels, it would be 3.\n",
        "-   A convolutional layer is like a window that scans over the image,\n",
        "    looking for a pattern it recognizes. These patterns are called\n",
        "    *features,* and one of the parameters of a convolutional layer is\n",
        "    the number of features we would like it to learn. **This is the\n",
        "    second argument to the constructor is the number of output\n",
        "    features.** Here, we're asking our layer to learn 6 features.\n",
        "-   Just above, I likened the convolutional layer to a window - but how\n",
        "    big is the window? **The third argument is the window or kernel\n",
        "    size.** Here, the \"5\" means we've chosen a 5x5 kernel. (If you want\n",
        "    a kernel with height different from width, you can specify a tuple\n",
        "    for this argument - e.g., `(3, 5)` to get a 3x5 convolution kernel.)\n",
        "\n",
        "The output of a convolutional layer is an *activation map* - a spatial\n",
        "representation of the presence of features in the input tensor. `conv1`\n",
        "will give us an output tensor of 6x28x28; 6 is the number of features,\n",
        "and 28 is the height and width of our map. (The 28 comes from the fact\n",
        "that when scanning a 5-pixel window over a 32-pixel row, there are only\n",
        "28 valid positions.)\n",
        "\n",
        "We then pass the output of the convolution through a ReLU activation\n",
        "function (more on activation functions later), then through a max\n",
        "pooling layer. The max pooling layer takes features near each other in\n",
        "the activation map and groups them together. It does this by reducing\n",
        "the tensor, merging every 2x2 group of cells in the output into a single\n",
        "cell, and assigning that cell the maximum value of the 4 cells that went\n",
        "into it. This gives us a lower-resolution version of the activation map,\n",
        "with dimensions 6x14x14.\n",
        "\n",
        "Our next convolutional layer, `conv2`, expects 6 input channels\n",
        "(corresponding to the 6 features sought by the first layer), has 16\n",
        "output channels, and a 3x3 kernel. It puts out a 16x12x12 activation\n",
        "map, which is again reduced by a max pooling layer to 16x6x6. Prior to\n",
        "passing this output to the linear layers, it is reshaped to a 16 \\* 6 \\*\n",
        "6 = 576-element vector for consumption by the next layer.\n",
        "\n",
        "There are convolutional layers for addressing 1D, 2D, and 3D tensors.\n",
        "There are also many more optional arguments for a conv layer\n",
        "constructor, including stride length(e.g., only scanning every second or\n",
        "every third position) in the input, padding (so you can scan out to the\n",
        "edges of the input), and more. See the\n",
        "[documentation](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n",
        "for more information.\n",
        "\n",
        "Recurrent Layers\n",
        "================\n",
        "\n",
        "*Recurrent neural networks* (or *RNNs)* are used for sequential data\n",
        "-anything from time-series measurements from a scientific instrument to\n",
        "natural language sentences to DNA nucleotides. An RNN does this by\n",
        "maintaining a *hidden state* that acts as a sort of memory for what it\n",
        "has seen in the sequence so far.\n",
        "\n",
        "The internal structure of an RNN layer - or its variants, the LSTM (long\n",
        "short-term memory) and GRU (gated recurrent unit) - is moderately\n",
        "complex and beyond the scope of this video, but we'll show you what one\n",
        "looks like in action with an LSTM-based part-of-speech tagger (a type of\n",
        "classifier that tells you if a word is a noun, verb, etc.):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "2gtD9LeUQsZH"
      },
      "outputs": [],
      "source": [
        "class LSTMTagger(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtRhdMydQsZH"
      },
      "source": [
        "The constructor has four arguments:\n",
        "\n",
        "-   `vocab_size` is the number of words in the input vocabulary. Each\n",
        "    word is a one-hot vector (or unit vector) in a\n",
        "    `vocab_size`-dimensional space.\n",
        "-   `tagset_size` is the number of tags in the output set.\n",
        "-   `embedding_dim` is the size of the *embedding* space for the\n",
        "    vocabulary. An embedding maps a vocabulary onto a low-dimensional\n",
        "    space, where words with similar meanings are close together in the\n",
        "    space.\n",
        "-   `hidden_dim` is the size of the LSTM's memory.\n",
        "\n",
        "The input will be a sentence with the words represented as indices of\n",
        "one-hot vectors. The embedding layer will then map these down to an\n",
        "`embedding_dim`-dimensional space. The LSTM takes this sequence of\n",
        "embeddings and iterates over it, fielding an output vector of length\n",
        "`hidden_dim`. The final linear layer acts as a classifier; applying\n",
        "`log_softmax()` to the output of the final layer converts the output\n",
        "into a normalized set of estimated probabilities that a given word maps\n",
        "to a given tag.\n",
        "\n",
        "If you'd like to see this network in action, check out the [Sequence\n",
        "Models and LSTM\n",
        "Networks](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)\n",
        "tutorial on pytorch.org.\n",
        "\n",
        "Transformers\n",
        "============\n",
        "\n",
        "*Transformers* are multi-purpose networks that have taken over the state\n",
        "of the art in NLP with models like BERT. A discussion of transformer\n",
        "architecture is beyond the scope of this video, but PyTorch has a\n",
        "`Transformer` class that allows you to define the overall parameters of\n",
        "a transformer model - the number of attention heads, the number of\n",
        "encoder & decoder layers, dropout and activation functions, etc. (You\n",
        "can even build the BERT model from this single class, with the right\n",
        "parameters!) The `torch.nn.Transformer` class also has classes to\n",
        "encapsulate the individual components (`TransformerEncoder`,\n",
        "`TransformerDecoder`) and subcomponents (`TransformerEncoderLayer`,\n",
        "`TransformerDecoderLayer`). For details, check out the\n",
        "[documentation](https://pytorch.org/docs/stable/nn.html#transformer-layers)\n",
        "on transformer classes.\n",
        "\n",
        "Other Layers and Functions\n",
        "--------------------------\n",
        "\n",
        "Data Manipulation Layers\n",
        "========================\n",
        "\n",
        "There are other layer types that perform important functions in models,\n",
        "but don't participate in the learning process themselves.\n",
        "\n",
        "**Max pooling** (and its twin, min pooling) reduce a tensor by combining\n",
        "cells, and assigning the maximum value of the input cells to the output\n",
        "cell (we saw this). For example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuSDCn_zQsZH",
        "outputId": "1fce9fb2-8a67-4509-dfa1-485eacebacbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.8724, 0.0521, 0.5234, 0.2712, 0.8075, 0.6849],\n",
            "         [0.6205, 0.8893, 0.3989, 0.0695, 0.6793, 0.9779],\n",
            "         [0.7896, 0.1310, 0.6721, 0.1847, 0.5465, 0.4129],\n",
            "         [0.7210, 0.7140, 0.5548, 0.6181, 0.5909, 0.7307],\n",
            "         [0.3257, 0.0643, 0.4201, 0.7515, 0.9315, 0.5707],\n",
            "         [0.2225, 0.0919, 0.5892, 0.4374, 0.4969, 0.8219]]])\n",
            "tensor([[[0.8893, 0.9779],\n",
            "         [0.7210, 0.9315]]])\n"
          ]
        }
      ],
      "source": [
        "my_tensor = torch.rand(1, 6, 6)\n",
        "print(my_tensor)\n",
        "\n",
        "maxpool_layer = torch.nn.MaxPool2d(3)\n",
        "print(maxpool_layer(my_tensor))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPaHsAs8QsZH"
      },
      "source": [
        "If you look closely at the values above, you'll see that each of the\n",
        "values in the maxpooled output is the maximum value of each quadrant of\n",
        "the 6x6 input.\n",
        "\n",
        "**Normalization layers** re-center and normalize the output of one layer\n",
        "before feeding it to another. Centering and scaling the intermediate\n",
        "tensors has a number of beneficial effects, such as letting you use\n",
        "higher learning rates without exploding/vanishing gradients.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU07nNnrQsZH",
        "outputId": "516b9705-9ad0-404a-ca94-28308017a261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[14.0815,  7.7938, 18.1404, 13.3007],\n",
            "         [19.6295, 17.2427, 16.1859, 13.0774],\n",
            "         [15.5525, 11.4228, 12.4842, 23.5243],\n",
            "         [16.1281, 18.5413,  6.0246, 10.3449]]])\n",
            "tensor(14.5922)\n",
            "tensor([[[ 0.2041, -1.5016,  1.3052, -0.0077],\n",
            "         [ 1.3154,  0.3012, -0.1479, -1.4687],\n",
            "         [-0.0408, -0.9121, -0.6881,  1.6410],\n",
            "         [ 0.6877,  1.1804, -1.3750, -0.4930]]],\n",
            "       grad_fn=<NativeBatchNormBackward0>)\n",
            "tensor(7.4506e-09, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "my_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
        "print(my_tensor)\n",
        "\n",
        "print(my_tensor.mean())\n",
        "\n",
        "norm_layer = torch.nn.BatchNorm1d(4)\n",
        "normed_tensor = norm_layer(my_tensor)\n",
        "print(normed_tensor)\n",
        "\n",
        "print(normed_tensor.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j9JqU6BQsZI"
      },
      "source": [
        "Running the cell above, we've added a large scaling factor and offset to\n",
        "an input tensor; you should see the input tensor's `mean()` somewhere in\n",
        "the neighborhood of 15. After running it through the normalization\n",
        "layer, you can see that the values are smaller, and grouped around zero\n",
        "- in fact, the mean should be very small (\\> 1e-8).\n",
        "\n",
        "This is beneficial because many activation functions (discussed below)\n",
        "have their strongest gradients near 0, but sometimes suffer from\n",
        "vanishing or exploding gradients for inputs that drive them far away\n",
        "from zero. Keeping the data centered around the area of steepest\n",
        "gradient will tend to mean faster, better learning and higher feasible\n",
        "learning rates.\n",
        "\n",
        "**Dropout layers** are a tool for encouraging *sparse representations*\n",
        "in your model - that is, pushing it to do inference with less data.\n",
        "\n",
        "Dropout layers work by randomly setting parts of the input tensor\n",
        "*during training* - dropout layers are always turned off for inference.\n",
        "This forces the model to learn against this masked or reduced dataset.\n",
        "For example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pmsgUnYQsZI",
        "outputId": "9e5713e7-fa0f-4bb4-c330-93f3b1251451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.4737, 0.0000, 1.6428, 0.0000],\n",
            "         [0.0000, 1.1499, 0.6046, 0.8485],\n",
            "         [0.0000, 0.0000, 1.2764, 0.0000],\n",
            "         [0.0000, 0.9891, 0.6166, 0.5261]]])\n",
            "tensor([[[0.4737, 0.0000, 0.0000, 0.1840],\n",
            "         [0.0000, 1.1499, 0.6046, 0.0000],\n",
            "         [1.5198, 0.0000, 0.0000, 1.2674],\n",
            "         [0.2531, 0.0000, 0.0000, 0.5261]]])\n"
          ]
        }
      ],
      "source": [
        "my_tensor = torch.rand(1, 4, 4)\n",
        "\n",
        "dropout = torch.nn.Dropout(p=0.4)\n",
        "print(dropout(my_tensor))\n",
        "print(dropout(my_tensor))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJcXkg-NQsZI"
      },
      "source": [
        "Above, you can see the effect of dropout on a sample tensor. You can use\n",
        "the optional `p` argument to set the probability of an individual weight\n",
        "dropping out; if you don't it defaults to 0.5.\n",
        "\n",
        "Activation Functions\n",
        "====================\n",
        "\n",
        "Activation functions make deep learning possible. A neural network is\n",
        "really a program - with many parameters - that *simulates a mathematical\n",
        "function*. If all we did was multiple tensors by layer weights\n",
        "repeatedly, we could only simulate *linear functions;* further, there\n",
        "would be no point to having many layers, as the whole network would\n",
        "reduce could be reduced to a single matrix multiplication. Inserting\n",
        "*non-linear* activation functions between layers is what allows a deep\n",
        "learning model to simulate any function, rather than just linear ones.\n",
        "\n",
        "`torch.nn.Module` has objects encapsulating all of the major activation\n",
        "functions including ReLU and its many variants, Tanh, Hardtanh, sigmoid,\n",
        "and more. It also includes other functions, such as Softmax, that are\n",
        "most useful at the output stage of a model.\n",
        "\n",
        "Loss Functions\n",
        "==============\n",
        "\n",
        "Loss functions tell us how far a model's prediction is from the correct\n",
        "answer. PyTorch contains a variety of loss functions, including common\n",
        "MSE (mean squared error = L2 norm), Cross Entropy Loss and Negative\n",
        "Likelihood Loss (useful for classifiers), and others.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "oZPmwv0rUyQq"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USX31R2HUyQs"
      },
      "source": [
        "[Introduction](introyt1_tutorial.html) \\|\\|\n",
        "[Tensors](tensors_deeper_tutorial.html) \\|\\|\n",
        "[Autograd](autogradyt_tutorial.html) \\|\\| [Building\n",
        "Models](modelsyt_tutorial.html) \\|\\| **TensorBoard Support** \\|\\|\n",
        "[Training Models](trainingyt.html) \\|\\| [Model\n",
        "Understanding](captumyt.html)\n",
        "\n",
        "PyTorch TensorBoard Support\n",
        "===========================\n",
        "\n",
        "Follow along with the video below or on\n",
        "[youtube](https://www.youtube.com/watch?v=6CEld3hZgqc).\n",
        "\n",
        "``` {.python .jupyter-code-cell}\n",
        "from IPython.display import display, HTML\n",
        "html_code = \"\"\"\n",
        "<div style=\"margin-top:10px; margin-bottom:10px;\">\n",
        "  <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6CEld3hZgqc\" frameborder=\"0\" allow=\"accelerometer; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
        "</div>\n",
        "\"\"\"\n",
        "display(HTML(html_code))\n",
        "```\n",
        "\n",
        "Before You Start\n",
        "----------------\n",
        "\n",
        "To run this tutorial, you'll need to install PyTorch, TorchVision,\n",
        "Matplotlib, and TensorBoard.\n",
        "\n",
        "With `conda`:\n",
        "\n",
        "``` {.sh}\n",
        "conda install pytorch torchvision -c pytorch\n",
        "conda install matplotlib tensorboard\n",
        "```\n",
        "\n",
        "With `pip`:\n",
        "\n",
        "``` {.sh}\n",
        "pip install torch torchvision matplotlib tensorboard\n",
        "```\n",
        "\n",
        "Once the dependencies are installed, restart this notebook in the Python\n",
        "environment where you installed them.\n",
        "\n",
        "Introduction\n",
        "------------\n",
        "\n",
        "In this notebook, we'll be training a variant of LeNet-5 against the\n",
        "Fashion-MNIST dataset. Fashion-MNIST is a set of image tiles depicting\n",
        "various garments, with ten class labels indicating the type of garment\n",
        "depicted.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64w6_XR3UyQt"
      },
      "outputs": [],
      "source": [
        "# PyTorch model and training necessities\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Image datasets and image manipulation\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Image display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# PyTorch TensorBoard support\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# In case you are using an environment that has TensorFlow installed,\n",
        "# such as Google Colab, uncomment the following code to avoid\n",
        "# a bug with saving embeddings to your TensorBoard directory\n",
        "\n",
        "# import tensorflow as tf\n",
        "# import tensorboard as tb\n",
        "# tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydwEzSuIUyQt"
      },
      "source": [
        "Showing Images in TensorBoard\n",
        "=============================\n",
        "\n",
        "Let's start by adding sample images from our dataset to TensorBoard:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "dzCkHn4nUyQu",
        "outputId": "fc78c8f1-0f4b-4ade-e35e-16025f4e8b57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 11.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 201kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.78MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 23.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Gather datasets and prepare them for consumption\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Store separate training and validations splits in ./data\n",
        "training_set = torchvision.datasets.FashionMNIST('./data',\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=transform)\n",
        "validation_set = torchvision.datasets.FashionMNIST('./data',\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=transform)\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(training_set,\n",
        "                                              batch_size=4,\n",
        "                                              shuffle=True,\n",
        "                                              num_workers=2)\n",
        "\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(validation_set,\n",
        "                                                batch_size=4,\n",
        "                                                shuffle=False,\n",
        "                                                num_workers=2)\n",
        "\n",
        "# Class labels\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
        "\n",
        "# Helper function for inline image display\n",
        "def matplotlib_imshow(img, one_channel=False):\n",
        "    if one_channel:\n",
        "        img = img.mean(dim=0)\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    if one_channel:\n",
        "        plt.imshow(npimg, cmap=\"Greys\")\n",
        "    else:\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "# Extract a batch of 4 images\n",
        "dataiter = iter(training_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Create a grid from the images and show them\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "matplotlib_imshow(img_grid, one_channel=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bk-9LoNUyQu"
      },
      "source": [
        "Above, we used TorchVision and Matplotlib to create a visual grid of a\n",
        "minibatch of our input data. Below, we use the `add_image()` call on\n",
        "`SummaryWriter` to log the image for consumption by TensorBoard, and we\n",
        "also call `flush()` to make sure it's written to disk right away.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "I6u2PvnXUyQu"
      },
      "outputs": [],
      "source": [
        "# Default log_dir argument is \"runs\" - but it's good to be specific\n",
        "# torch.utils.tensorboard.SummaryWriter is imported above\n",
        "writer = SummaryWriter('runs/fashion_mnist_experiment_1')\n",
        "\n",
        "# Write image data to TensorBoard log dir\n",
        "writer.add_image('Four Fashion-MNIST Images', img_grid)\n",
        "writer.flush()\n",
        "\n",
        "# To view, start TensorBoard on the command line with:\n",
        "#   tensorboard --logdir=runs\n",
        "# ...and open a browser tab to http://localhost:6006/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUdR6hPQUyQu"
      },
      "source": [
        "If you start TensorBoard at the command line and open it in a new\n",
        "browser tab (usually at [localhost:6006](localhost:6006)), you should\n",
        "see the image grid under the IMAGES tab.\n",
        "\n",
        "Graphing Scalars to Visualize Training\n",
        "======================================\n",
        "\n",
        "TensorBoard is useful for tracking the progress and efficacy of your\n",
        "training. Below, we'll run a training loop, track some metrics, and save\n",
        "the data for TensorBoard's consumption.\n",
        "\n",
        "Let's define a model to categorize our image tiles, and an optimizer and\n",
        "loss function for training:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "Ndr8Ic_PUyQv"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbKu8gP1UyQv"
      },
      "source": [
        "Now let's train a single epoch, and evaluate the training vs. validation\n",
        "set losses every 1000 batches:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwHWxmUOUyQv",
        "outputId": "655f4ada-963f-4835-dd3c-843d77215c15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500\n",
            "Batch 1000\n",
            "Batch 2000\n",
            "Batch 3000\n",
            "Batch 4000\n",
            "Batch 5000\n",
            "Batch 6000\n",
            "Batch 7000\n",
            "Batch 8000\n",
            "Batch 9000\n",
            "Batch 10000\n",
            "Batch 11000\n",
            "Batch 12000\n",
            "Batch 13000\n",
            "Batch 14000\n",
            "Batch 15000\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "print(len(validation_loader))\n",
        "for epoch in range(1):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(training_loader, 0):\n",
        "        # basic training loop\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:    # Every 1000 mini-batches...\n",
        "            print('Batch {}'.format(i + 1))\n",
        "            # Check against the validation set\n",
        "            running_vloss = 0.0\n",
        "\n",
        "            # In evaluation mode some model specific operations can be omitted eg. dropout layer\n",
        "            net.train(False) # Switching to evaluation mode, eg. turning off regularisation\n",
        "            for j, vdata in enumerate(validation_loader, 0):\n",
        "                vinputs, vlabels = vdata\n",
        "                voutputs = net(vinputs)\n",
        "                vloss = criterion(voutputs, vlabels)\n",
        "                running_vloss += vloss.item()\n",
        "            net.train(True) # Switching back to training mode, eg. turning on regularisation\n",
        "\n",
        "            avg_loss = running_loss / 1000\n",
        "            avg_vloss = running_vloss / len(validation_loader)\n",
        "\n",
        "            # Log the running loss averaged per batch\n",
        "            writer.add_scalars('Training vs. Validation Loss',\n",
        "                            { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                            epoch * len(training_loader) + i)\n",
        "\n",
        "            running_loss = 0.0\n",
        "print('Finished Training')\n",
        "\n",
        "writer.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q93FVpqRUyQv"
      },
      "source": [
        "Switch to your open TensorBoard and have a look at the SCALARS tab.\n",
        "\n",
        "Visualizing Your Model\n",
        "======================\n",
        "\n",
        "TensorBoard can also be used to examine the data flow within your model.\n",
        "To do this, call the `add_graph()` method with a model and sample input:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "JAY9U6_ZUyQw"
      },
      "outputs": [],
      "source": [
        "# Again, grab a single mini-batch of images\n",
        "dataiter = iter(training_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# add_graph() will trace the sample input through your model,\n",
        "# and render it as a graph.\n",
        "writer.add_graph(net, images)\n",
        "writer.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCjaVLrBUyQw"
      },
      "source": [
        "When you switch over to TensorBoard, you should see a GRAPHS tab.\n",
        "Double-click the \"NET\" node to see the layers and data flow within your\n",
        "model.\n",
        "\n",
        "Visualizing Your Dataset with Embeddings\n",
        "========================================\n",
        "\n",
        "The 28-by-28 image tiles we're using can be modeled as 784-dimensional\n",
        "vectors (28 \\* 28 = 784). It can be instructive to project this to a\n",
        "lower-dimensional representation. The `add_embedding()` method will\n",
        "project a set of data onto the three dimensions with highest variance,\n",
        "and display them as an interactive 3D chart. The `add_embedding()`\n",
        "method does this automatically by projecting to the three dimensions\n",
        "with highest variance.\n",
        "\n",
        "Below, we'll take a sample of our data, and generate such an embedding:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "qiJ2b-Z9UyQw"
      },
      "outputs": [],
      "source": [
        "# Select a random subset of data and corresponding labels\n",
        "def select_n_random(data, labels, n=100):\n",
        "    assert len(data) == len(labels)\n",
        "\n",
        "    perm = torch.randperm(len(data))\n",
        "    return data[perm][:n], labels[perm][:n]\n",
        "\n",
        "# Extract a random subset of data\n",
        "images, labels = select_n_random(training_set.data, training_set.targets)\n",
        "\n",
        "# get the class labels for each image\n",
        "class_labels = [classes[label] for label in labels]\n",
        "\n",
        "# log embeddings\n",
        "features = images.view(-1, 28 * 28)\n",
        "writer.add_embedding(features,\n",
        "                    metadata=class_labels,\n",
        "                    label_img=images.unsqueeze(1))\n",
        "writer.flush()\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coiViBNuUyQw"
      },
      "source": [
        "Now if you switch to TensorBoard and select the PROJECTOR tab, you\n",
        "should see a 3D representation of the projection. You can rotate and\n",
        "zoom the model. Examine it at large and small scales, and see whether\n",
        "you can spot patterns in the projected data and the clustering of\n",
        "labels.\n",
        "\n",
        "For better visibility, it's recommended to:\n",
        "\n",
        "-   Select \"label\" from the \"Color by\" drop-down on the left.\n",
        "-   Toggle the Night Mode icon along the top to place the light-colored\n",
        "    images on a dark background.\n",
        "\n",
        "Other Resources\n",
        "===============\n",
        "\n",
        "For more information, have a look at:\n",
        "\n",
        "-   PyTorch documentation on\n",
        "    [torch.utils.tensorboard.SummaryWriter](https://pytorch.org/docs/stable/tensorboard.html?highlight=summarywriter)\n",
        "-   Tensorboard tutorial content in the [PyTorch.org\n",
        "    Tutorials](https://pytorch.org/tutorials/)\n",
        "-   For more information about TensorBoard, see the [TensorBoard\n",
        "    documentation](https://www.tensorflow.org/tensorboard)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "qCeLAwOcWNWB"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Z1tULqWNWD"
      },
      "source": [
        "[Introduction](introyt1_tutorial.html) \\|\\|\n",
        "[Tensors](tensors_deeper_tutorial.html) \\|\\|\n",
        "[Autograd](autogradyt_tutorial.html) \\|\\| [Building\n",
        "Models](modelsyt_tutorial.html) \\|\\| [TensorBoard\n",
        "Support](tensorboardyt_tutorial.html) \\|\\| **Training Models** \\|\\|\n",
        "[Model Understanding](captumyt.html)\n",
        "\n",
        "Training with PyTorch\n",
        "=====================\n",
        "\n",
        "Follow along with the video below or on\n",
        "[youtube](https://www.youtube.com/watch?v=jF43_wj_DCQ).\n",
        "\n",
        "``` {.python .jupyter-code-cell}\n",
        "from IPython.display import display, HTML\n",
        "html_code = \"\"\"\n",
        "<div style=\"margin-top:10px; margin-bottom:10px;\">\n",
        "  <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/jF43_wj_DCQ\" frameborder=\"0\" allow=\"accelerometer; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
        "</div>\n",
        "\"\"\"\n",
        "display(HTML(html_code))\n",
        "```\n",
        "\n",
        "Introduction\n",
        "------------\n",
        "\n",
        "In past videos, we've discussed and demonstrated:\n",
        "\n",
        "-   Building models with the neural network layers and functions of the\n",
        "    torch.nn module\n",
        "-   The mechanics of automated gradient computation, which is central to\n",
        "    gradient-based model training\n",
        "-   Using TensorBoard to visualize training progress and other\n",
        "    activities\n",
        "\n",
        "In this video, we'll be adding some new tools to your inventory:\n",
        "\n",
        "-   We'll get familiar with the dataset and dataloader abstractions, and\n",
        "    how they ease the process of feeding data to your model during a\n",
        "    training loop\n",
        "-   We'll discuss specific loss functions and when to use them\n",
        "-   We'll look at PyTorch optimizers, which implement algorithms to\n",
        "    adjust model weights based on the outcome of a loss function\n",
        "\n",
        "Finally, we'll pull all of these together and see a full PyTorch\n",
        "training loop in action.\n",
        "\n",
        "Dataset and DataLoader\n",
        "----------------------\n",
        "\n",
        "The `Dataset` and `DataLoader` classes encapsulate the process of\n",
        "pulling your data from storage and exposing it to your training loop in\n",
        "batches.\n",
        "\n",
        "The `Dataset` is responsible for accessing and processing single\n",
        "instances of data.\n",
        "\n",
        "The `DataLoader` pulls instances of data from the `Dataset` (either\n",
        "automatically or with a sampler that you define), collects them in\n",
        "batches, and returns them for consumption by your training loop. The\n",
        "`DataLoader` works with all kinds of datasets, regardless of the type of\n",
        "data they contain.\n",
        "\n",
        "For this tutorial, we'll be using the Fashion-MNIST dataset provided by\n",
        "TorchVision. We use `torchvision.transforms.Normalize()` to zero-center\n",
        "and normalize the distribution of the image tile content, and download\n",
        "both training and validation data splits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sko3shI0WNWE",
        "outputId": "2273ae37-d307-496d-9eea-c58b91aa26be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set has 60000 instances\n",
            "Validation set has 10000 instances\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# PyTorch TensorBoard support\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Create datasets for training & validation, download if necessary\n",
        "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
        "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# Create data loaders for our datasets; shuffle for training, not for validation\n",
        "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n",
        "\n",
        "# Class labels\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
        "\n",
        "# Report split sizes\n",
        "print('Training set has {} instances'.format(len(training_set)))\n",
        "print('Validation set has {} instances'.format(len(validation_set)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ksazrXeWNWF"
      },
      "source": [
        "As always, let's visualize the data as a sanity check:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "ZL-oTJJXWNWF",
        "outputId": "a123d43d-6c30-4509-8de0-44efc491e8cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag  Coat  Pullover  Bag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Helper function for inline image display\n",
        "def matplotlib_imshow(img, one_channel=False):\n",
        "    if one_channel:\n",
        "        img = img.mean(dim=0)\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    if one_channel:\n",
        "        plt.imshow(npimg, cmap=\"Greys\")\n",
        "    else:\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "dataiter = iter(training_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Create a grid from the images and show them\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "matplotlib_imshow(img_grid, one_channel=True)\n",
        "print('  '.join(classes[labels[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFmyyi9VWNWF"
      },
      "source": [
        "The Model\n",
        "=========\n",
        "\n",
        "The model we'll use in this example is a variant of LeNet-5 - it should\n",
        "be familiar if you've watched the previous videos in this series.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "kYaqGspVWNWF"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# PyTorch models inherit from torch.nn.Module\n",
        "class GarmentClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GarmentClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = GarmentClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmYXRneUWNWG"
      },
      "source": [
        "Loss Function\n",
        "=============\n",
        "\n",
        "For this example, we'll be using a cross-entropy loss. For demonstration\n",
        "purposes, we'll create batches of dummy output and label values, run\n",
        "them through the loss function, and examine the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUjlL7kEWNWG",
        "outputId": "a4ec2fad-3c72-4ae6-dcae-57f4acaaa62a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4753, 0.6406, 0.3770, 0.2260, 0.3039, 0.7103, 0.3736, 0.6462, 0.6525,\n",
            "         0.1568],\n",
            "        [0.8526, 0.0800, 0.8026, 0.8590, 0.1236, 0.7815, 0.5574, 0.2931, 0.8521,\n",
            "         0.0931],\n",
            "        [0.2438, 0.6945, 0.0434, 0.3200, 0.6681, 0.7111, 0.3049, 0.3037, 0.3907,\n",
            "         0.7617],\n",
            "        [0.1717, 0.2951, 0.8188, 0.1211, 0.2832, 0.9372, 0.6476, 0.1979, 0.7552,\n",
            "         0.5081]])\n",
            "tensor([1, 5, 3, 7])\n",
            "Total loss for this batch: 2.3274948596954346\n"
          ]
        }
      ],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# NB: Loss functions expect data in batches, so we're creating batches of 4\n",
        "# Represents the model's confidence in each of the 10 classes for a given input\n",
        "dummy_outputs = torch.rand(4, 10)\n",
        "# Represents the correct class among the 10 being tested\n",
        "dummy_labels = torch.tensor([1, 5, 3, 7])\n",
        "\n",
        "print(dummy_outputs)\n",
        "print(dummy_labels)\n",
        "\n",
        "loss = loss_fn(dummy_outputs, dummy_labels)\n",
        "print('Total loss for this batch: {}'.format(loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB2owJoWWNWG"
      },
      "source": [
        "Optimizer\n",
        "=========\n",
        "\n",
        "For this example, we'll be using simple [stochastic gradient\n",
        "descent](https://pytorch.org/docs/stable/optim.html) with momentum.\n",
        "\n",
        "It can be instructive to try some variations on this optimization\n",
        "scheme:\n",
        "\n",
        "-   Learning rate determines the size of the steps the optimizer takes.\n",
        "    What does a different learning rate do to the your training results,\n",
        "    in terms of accuracy and convergence time?\n",
        "-   Momentum nudges the optimizer in the direction of strongest gradient\n",
        "    over multiple steps. What does changing this value do to your\n",
        "    results?\n",
        "-   Try some different optimization algorithms, such as averaged SGD,\n",
        "    Adagrad, or Adam. How do your results differ?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "yiRfZHJ4WNWG"
      },
      "outputs": [],
      "source": [
        "# Optimizers specified in the torch.optim package\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reVjfxgWWNWH"
      },
      "source": [
        "The Training Loop\n",
        "=================\n",
        "\n",
        "Below, we have a function that performs one training epoch. It\n",
        "enumerates data from the DataLoader, and on each pass of the loop does\n",
        "the following:\n",
        "\n",
        "-   Gets a batch of training data from the DataLoader\n",
        "-   Zeros the optimizer's gradients\n",
        "-   Performs an inference - that is, gets predictions from the model for\n",
        "    an input batch\n",
        "-   Calculates the loss for that set of predictions vs. the labels on\n",
        "    the dataset\n",
        "-   Calculates the backward gradients over the learning weights\n",
        "-   Tells the optimizer to perform one learning step - that is, adjust\n",
        "    the model's learning weights based on the observed gradients for\n",
        "    this batch, according to the optimization algorithm we chose\n",
        "-   It reports on the loss for every 1000 batches.\n",
        "-   Finally, it reports the average per-batch loss for the last 1000\n",
        "    batches, for comparison with a validation run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "QM6CK4FqWNWH"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(training_loader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:\n",
        "            last_loss = running_loss / 1000 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            tb_x = epoch_index * len(training_loader) + i + 1\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zslEafOOWNWH"
      },
      "source": [
        "Per-Epoch Activity\n",
        "==================\n",
        "\n",
        "There are a couple of things we'll want to do once per epoch:\n",
        "\n",
        "-   Perform validation by checking our relative loss on a set of data\n",
        "    that was not used for training, and report this\n",
        "-   Save a copy of the model\n",
        "\n",
        "Here, we'll do our reporting in TensorBoard. This will require going to\n",
        "the command line to start TensorBoard, and opening it in another browser\n",
        "tab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWU00BvRWNWH",
        "outputId": "b40dbcc7-87b9-436a-f442-b3462baa64d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n",
            "  batch 1000 loss: 1.2407228263523429\n",
            "  batch 2000 loss: 0.7447262428709073\n",
            "  batch 3000 loss: 0.620096411830149\n",
            "  batch 4000 loss: 0.5647598720234164\n",
            "  batch 5000 loss: 0.5412475800080574\n",
            "  batch 6000 loss: 0.5231343562626221\n",
            "  batch 7000 loss: 0.4947164810871618\n",
            "  batch 8000 loss: 0.48562994779022844\n",
            "  batch 9000 loss: 0.4967561713561299\n",
            "  batch 10000 loss: 0.46601515697391005\n",
            "  batch 11000 loss: 0.5022053881804895\n",
            "  batch 12000 loss: 0.4480734685155949\n",
            "  batch 13000 loss: 0.4394713856492917\n",
            "  batch 14000 loss: 0.41659240305376444\n",
            "  batch 15000 loss: 0.4477725848404516\n",
            "LOSS train 0.4477725848404516 valid 0.44510766863822937\n",
            "EPOCH 2:\n",
            "  batch 1000 loss: 0.43757492329342856\n",
            "  batch 2000 loss: 0.43847297872603214\n",
            "  batch 3000 loss: 0.4205772340402673\n",
            "  batch 4000 loss: 0.4392407243750672\n",
            "  batch 5000 loss: 0.4428412042823093\n",
            "  batch 6000 loss: 0.43055014562011024\n",
            "  batch 7000 loss: 0.421192861450746\n",
            "  batch 8000 loss: 0.43543081308624915\n",
            "  batch 9000 loss: 0.4359343656747485\n",
            "  batch 10000 loss: 0.3907226020850138\n",
            "  batch 11000 loss: 0.4014303638163562\n",
            "  batch 12000 loss: 0.42206602667072\n",
            "  batch 13000 loss: 0.40132832458401346\n",
            "  batch 14000 loss: 0.38497698441160877\n",
            "  batch 15000 loss: 0.4017669109104595\n",
            "LOSS train 0.4017669109104595 valid 0.4246806502342224\n",
            "EPOCH 3:\n",
            "  batch 1000 loss: 0.3886701632136676\n",
            "  batch 2000 loss: 0.40931374051882086\n",
            "  batch 3000 loss: 0.38926714051833644\n",
            "  batch 4000 loss: 0.3750786704349859\n",
            "  batch 5000 loss: 0.3986806833298251\n",
            "  batch 6000 loss: 0.3782163248673951\n",
            "  batch 7000 loss: 0.3826220282979407\n",
            "  batch 8000 loss: 0.414090723106523\n",
            "  batch 9000 loss: 0.3582681281080437\n",
            "  batch 10000 loss: 0.3976680791123781\n",
            "  batch 11000 loss: 0.3868282643767133\n",
            "  batch 12000 loss: 0.38914940632669426\n",
            "  batch 13000 loss: 0.3738800481072692\n",
            "  batch 14000 loss: 0.4000602638237415\n",
            "  batch 15000 loss: 0.40576752039153335\n",
            "LOSS train 0.40576752039153335 valid 0.48210474848747253\n",
            "EPOCH 4:\n",
            "  batch 1000 loss: 0.3767613225350253\n",
            "  batch 2000 loss: 0.36555408268521367\n",
            "  batch 3000 loss: 0.3663115859927998\n",
            "  batch 4000 loss: 0.3896344967799877\n",
            "  batch 5000 loss: 0.38999960119922705\n",
            "  batch 6000 loss: 0.3705446690059039\n",
            "  batch 7000 loss: 0.3762089968620583\n",
            "  batch 8000 loss: 0.40330966244289823\n",
            "  batch 9000 loss: 0.3717223956813851\n",
            "  batch 10000 loss: 0.3634884573069121\n",
            "  batch 11000 loss: 0.36339179249014003\n",
            "  batch 12000 loss: 0.3909611837452854\n",
            "  batch 13000 loss: 0.35965683472831145\n",
            "  batch 14000 loss: 0.37414518036280114\n",
            "  batch 15000 loss: 0.3676315444568822\n",
            "LOSS train 0.3676315444568822 valid 0.45128169655799866\n",
            "EPOCH 5:\n",
            "  batch 1000 loss: 0.34796217759230863\n",
            "  batch 2000 loss: 0.3602654289946665\n",
            "  batch 3000 loss: 0.36099091703278147\n",
            "  batch 4000 loss: 0.35408780242273735\n",
            "  batch 5000 loss: 0.39879662548054395\n",
            "  batch 6000 loss: 0.36377085969760437\n",
            "  batch 7000 loss: 0.35676533435107644\n",
            "  batch 8000 loss: 0.36787230979711544\n",
            "  batch 9000 loss: 0.37802856622114905\n",
            "  batch 10000 loss: 0.39079672460106213\n",
            "  batch 11000 loss: 0.3627331130133446\n",
            "  batch 12000 loss: 0.39027069804150394\n",
            "  batch 13000 loss: 0.39672236065694255\n",
            "  batch 14000 loss: 0.37418433097334963\n",
            "  batch 15000 loss: 0.4000975182307741\n",
            "LOSS train 0.4000975182307741 valid 0.4119134843349457\n",
            "EPOCH 6:\n",
            "  batch 1000 loss: 0.3480170789549586\n",
            "  batch 2000 loss: 0.3854119009249293\n",
            "  batch 3000 loss: 0.34476501912330115\n",
            "  batch 4000 loss: 0.34405651585620395\n",
            "  batch 5000 loss: 0.38435056176950066\n",
            "  batch 6000 loss: 0.3731388636252108\n",
            "  batch 7000 loss: 0.38833762551863277\n",
            "  batch 8000 loss: 0.3601046951780639\n",
            "  batch 9000 loss: 0.3776857011414531\n",
            "  batch 10000 loss: 0.4037272329714058\n",
            "  batch 11000 loss: 0.3687267378994129\n",
            "  batch 12000 loss: 0.3630641287803564\n",
            "  batch 13000 loss: 0.3881516797425263\n",
            "  batch 14000 loss: 0.39434053240283423\n",
            "  batch 15000 loss: 0.356654406280715\n",
            "LOSS train 0.356654406280715 valid 0.4154985845088959\n",
            "EPOCH 7:\n",
            "  batch 1000 loss: 0.33523746246569064\n",
            "  batch 2000 loss: 0.38175579631379764\n",
            "  batch 3000 loss: 0.3720834363844714\n",
            "  batch 4000 loss: 0.3611414759279311\n",
            "  batch 5000 loss: 0.38012857042822584\n",
            "  batch 6000 loss: 0.3857534642143655\n",
            "  batch 7000 loss: 0.34864419936590846\n",
            "  batch 8000 loss: 0.3545328563312259\n",
            "  batch 9000 loss: 0.3554392989020398\n",
            "  batch 10000 loss: 0.35878297804652626\n",
            "  batch 11000 loss: 0.3533826794004077\n",
            "  batch 12000 loss: 0.3861461588353616\n",
            "  batch 13000 loss: 0.37055541334257225\n",
            "  batch 14000 loss: 0.35860819685649675\n",
            "  batch 15000 loss: 0.39140348301709255\n",
            "LOSS train 0.39140348301709255 valid 0.42375701665878296\n",
            "EPOCH 8:\n",
            "  batch 1000 loss: 0.3464603700441337\n",
            "  batch 2000 loss: 0.3686634749450271\n",
            "  batch 3000 loss: 0.3458830914425242\n",
            "  batch 4000 loss: 0.3730962783300996\n",
            "  batch 5000 loss: 0.36998869571632725\n",
            "  batch 6000 loss: 0.3822425221699347\n",
            "  batch 7000 loss: 0.35353888107011333\n",
            "  batch 8000 loss: 0.3321226263211864\n",
            "  batch 9000 loss: 0.37515487017586563\n",
            "  batch 10000 loss: 0.36459593228886156\n",
            "  batch 11000 loss: 0.3703027243573272\n",
            "  batch 12000 loss: 0.3609847844354454\n",
            "  batch 13000 loss: 0.35516890714228977\n",
            "  batch 14000 loss: 0.3486767753055339\n",
            "  batch 15000 loss: 0.36581936627123646\n",
            "LOSS train 0.36581936627123646 valid 0.43409478664398193\n",
            "EPOCH 9:\n",
            "  batch 1000 loss: 0.36722395425171717\n",
            "  batch 2000 loss: 0.3591521037979274\n",
            "  batch 3000 loss: 0.3546775277478679\n",
            "  batch 4000 loss: 0.37539751392337184\n",
            "  batch 5000 loss: 0.35384413835240036\n",
            "  batch 6000 loss: 0.3701003686931863\n",
            "  batch 7000 loss: 0.3475863830386452\n",
            "  batch 8000 loss: 0.3549375646019149\n",
            "  batch 9000 loss: 0.33214021999303756\n",
            "  batch 10000 loss: 0.3859449540577934\n",
            "  batch 11000 loss: 0.3770578895435932\n",
            "  batch 12000 loss: 0.35791704311466\n",
            "  batch 13000 loss: 0.3598369216049113\n",
            "  batch 14000 loss: 0.3806263382470033\n",
            "  batch 15000 loss: 0.4008759014360069\n",
            "LOSS train 0.4008759014360069 valid 0.4157424867153168\n",
            "EPOCH 10:\n",
            "  batch 1000 loss: 0.38362518683413\n",
            "  batch 2000 loss: 0.366624843665558\n",
            "  batch 3000 loss: 0.3488114397036467\n",
            "  batch 4000 loss: 0.34589803378969264\n",
            "  batch 5000 loss: 0.36605924070210816\n",
            "  batch 6000 loss: 0.366577764748243\n",
            "  batch 7000 loss: 0.3700189734124615\n",
            "  batch 8000 loss: 0.34361861681604977\n",
            "  batch 9000 loss: 0.4064914622504168\n",
            "  batch 10000 loss: 0.4049768928407552\n",
            "  batch 11000 loss: 0.4260539794296201\n",
            "  batch 12000 loss: 0.42073677151285926\n",
            "  batch 13000 loss: 0.4042966708382404\n",
            "  batch 14000 loss: 0.39174736925415526\n",
            "  batch 15000 loss: 0.40237626881635963\n",
            "LOSS train 0.40237626881635963 valid 0.4351716935634613\n"
          ]
        }
      ],
      "source": [
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in enumerate(validation_loader):\n",
        "            vinputs, vlabels = vdata\n",
        "            voutputs = model(vinputs)\n",
        "            vloss = loss_fn(voutputs, vlabels)\n",
        "            running_vloss += vloss\n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GojuG9X0WNWH"
      },
      "source": [
        "To load a saved version of the model:\n",
        "\n",
        "``` {.python}\n",
        "saved_model = GarmentClassifier()\n",
        "saved_model.load_state_dict(torch.load(PATH))\n",
        "```\n",
        "\n",
        "Once you've loaded the model, it's ready for whatever you need it for\n",
        "-more training, inference, or analysis.\n",
        "\n",
        "Note that if your model has constructor parameters that affect model\n",
        "structure, you'll need to provide them and configure the model\n",
        "identically to the state in which it was saved.\n",
        "\n",
        "Other Resources\n",
        "===============\n",
        "\n",
        "-   Docs on the [data\n",
        "    utilities](https://pytorch.org/docs/stable/data.html), including\n",
        "    Dataset and DataLoader, at pytorch.org\n",
        "-   A [note on the use of pinned\n",
        "    memory](https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-pinning)\n",
        "    for GPU training\n",
        "-   Documentation on the datasets available in\n",
        "    [TorchVision](https://pytorch.org/vision/stable/datasets.html),\n",
        "    [TorchText](https://pytorch.org/text/stable/datasets.html), and\n",
        "    [TorchAudio](https://pytorch.org/audio/stable/datasets.html)\n",
        "-   Documentation on the [loss\n",
        "    functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "    available in PyTorch\n",
        "-   Documentation on the [torch.optim\n",
        "    package](https://pytorch.org/docs/stable/optim.html), which includes\n",
        "    optimizers and related tools, such as learning rate scheduling\n",
        "-   A detailed [tutorial on saving and loading\n",
        "    models](https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
        "-   The [Tutorials section of\n",
        "    pytorch.org](https://pytorch.org/tutorials/) contains tutorials on a\n",
        "    broad variety of training tasks, including classification in\n",
        "    different domains, generative adversarial networks, reinforcement\n",
        "    learning, and more\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}